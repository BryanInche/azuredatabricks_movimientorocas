{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3765a0f3-84d7-403e-973d-6c0c772524eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c13be4-a07f-49c3-89a3-300c3842105b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/nuevomodelo/modelrnn_nuevo2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2105079-aa8e-4c4f-8e80-337b67efeff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Paso 1: Copiar el modelo desde el almacenamiento Azure Storage al Directorio LOCAL(.OS) -- Obligatorio Poner file:\n",
    "dbutils.fs.cp(\"/mnt/datalakemlopsd4m/presentation/ModelsResults/modelornn_version1\", \"file:/nuevomodelo/modelrnn_nuevo3\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d68073-7b32-4875-bc9b-8567b1e9dd06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2: Cargar el Modelo ML desde el Directorio LOCAL(.OS) -- No es necesario poner file:\n",
    "model2 = tf.keras.models.load_model(\"/nuevomodelo/modelrnn_nuevo3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fea8ef2-4696-4712-83bd-e45a335ffb72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Paso 3: Hace la prediccion con el modelo cargado del Azure Storage\n",
    "model2.predict([[12,45,229,4,129]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf1c8a8a-ed39-4f5e-9841-7ec892e33675",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Especifica el nombre del modelo registrado en MLflow Registry\n",
    "model_name = \"MLops_kerastf_v1\"\n",
    "\n",
    "# Especifica la versión del modelo que deseas cargar\n",
    "model_version = \"1\"\n",
    "\n",
    "# Carga el modelo desde MLflow Registry especificando la versión\n",
    "keras_model = mlflow.keras.load_model(f\"models:/{model_name}/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6dcbfa-c3fa-40cc-ade3-d9f6d0ded7c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Datos de entrada\n",
    "input_data = np.array([[12, 35, 129, 4, 129]])\n",
    "\n",
    "# Remodelar los datos de entrada para que tengan la forma (None, 1, 5)\n",
    "input_data_reshaped = np.reshape(input_data, (input_data.shape[0], 1, input_data.shape[1]))\n",
    "\n",
    "\n",
    "keras_pred = keras_model.predict(input_data_reshaped)\n",
    "keras_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51eae32b-07da-42f0-b5dc-546edfd960d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "endpoint_name = \"mlops_kerasv1_endpoint\"\n",
    "model_name = \"MLops_kerastf_v1\"\n",
    "\n",
    "#ELEGIR UNA VERSION EN ESPECIFICO\n",
    "#specific_model_version = 1  # Especifica la versión deseada del modelo\n",
    "#Accede a la versión específica del modelo\n",
    "#model_version = MlflowClient().get_model_version(model_name, specific_model_version).version\n",
    "\n",
    "#ELEGIR LA ULTIMA VERSION DISPONIBLE\n",
    "model_version = MlflowClient().get_registered_model(model_name).latest_versions[0].version \n",
    "workload_type = \"GPU_LARGE\"  #tipo de carga de trabajo que se ejecutará en el endpoint de producción. Puede ser CPU, GPU_SMALL, GPU_LARGE\n",
    "workload_size = \"Small\"  # indica el tamaño del recurso de cómputo que se asignará al endpoint de producción. Puede ser Small, Medium, Large\n",
    "scale_to_zero = False  # Si se establece en True, el sistema puede apagar automáticamente el endpoint cuando no esté recibiendo solicitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f77ff8e-eaeb-4e10-b6f2-172345907745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "#apiUrl de la API que se utilizará para interactuar con el servicio de MLflow\n",
    "API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() \n",
    "\n",
    "#apiToken de autorización necesario para autenticar las solicitudes a la API\n",
    "API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()  \n",
    "\n",
    "#Cabeceras de la solicitud HTTP\n",
    "data = {\n",
    "    \"name\": endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"entity_name\": model_name,\n",
    "                \"entity_version\": model_version,\n",
    "                \"workload_size\": workload_size,\n",
    "                \"scale_to_zero_enabled\": scale_to_zero,\n",
    "                \"workload_type\": workload_type,\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "# Token de autorización \n",
    "headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "#Se realiza una solicitud POST a la URL de la API de MLflow para crear el endpoint de producción\n",
    "response = requests.post(\n",
    "    url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers\n",
    ")\n",
    "\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bf685c3-1aca-47f1-b38e-e3f37107c28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# send the POST request to create the serving endpoint\n",
    "\n",
    "# Datos de entrada\n",
    "input_data = np.array([[12, 35, 129, 4, 129]])\n",
    "\n",
    "# Remodelar los datos de entrada para que tengan la forma (None, 1, 5)\n",
    "input_data_reshaped = np.reshape(input_data, (input_data.shape[0], 1, input_data.shape[1]))\n",
    "\n",
    "# Convertir los datos de entrada a una lista de Python para serializarlos a JSON\n",
    "input_data_list = input_data_reshaped.tolist()\n",
    "\n",
    "data = {\n",
    "  \"inputs\" : input_data_list,\n",
    "  \"params\" : {\"max_new_tokens\": 100, \"temperature\": 1}\n",
    "}\n",
    "\n",
    "headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "response = requests.post(\n",
    "    url=f\"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations\", json=data, headers=headers\n",
    ")\n",
    "\n",
    "print(json.dumps(response.json()))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5.model_deploy",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
