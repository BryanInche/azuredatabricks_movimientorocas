{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95c7541f-3028-4e06-abf0-a15f656054ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Mount Azure Data Lake using Service Principal\n",
    "#### Steps to follow\n",
    "1. Get client_id, tenant_id and client_secret from key vault\n",
    "2. Set Spark Config with App/ Client Id, Directory/ Tenant Id & Secret\n",
    "3. Call file system utlity mount to mount the storage\n",
    "4. Explore other file system utlities related to mount (list all mounts, unmount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d98ed6-d9d0-49d8-a427-6fdc5bee19f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Obtenemos las credenciales necesarias para montar un almacenamiento en Microsoft Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615fd8ed-a1ea-465a-bc1d-88fc6306ee04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d7ac18-184c-44e2-b537-fc39818d91b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #Get client_id, tenant_id and client_secret from key vault\n",
    "# client_id = dbutils.secrets.get(scope = 'mlops-scope', key = 'mlops-app-client-id')\n",
    "# tenant_id = dbutils.secrets.get(scope = 'mlops-scope', key = 'mlops-app-tenant-id')\n",
    "# client_secret = dbutils.secrets.get(scope = 'mlops-scope', key = 'mlops-app-client-secret')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d02257b-5e44-4769-b514-da20ae34d99e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Asignamos las variables necesariamos para montar algun Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08529587-4086-4ccc-9c83-402e9c0601e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set Spark Config with App/ Client Id, Directory/ Tenant Id & Secret\n",
    "# configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "#           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "#           \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "#           \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "#           \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46fec388-7216-44bb-8405-95650be70164",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Montamos el almacenamiento de Data Lake que deseamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b57230-ec23-41d7-bba6-f3370b056215",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Call file system utlity mount to mount the storage\n",
    "# dbutils.fs.mount(\n",
    "#  source = \"abfss://presentation@datalakemlopsd4m.dfs.core.windows.net/\",\n",
    "#  mount_point = \"/mnt/datalakemlopsd4m/presentation/\",\n",
    "#  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17c1843-6f7e-4916-99f0-eb3bfe3be557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdd0054-ef8b-4d97-a7da-e92d8dbf199f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Verificar que contenedores estan montados en el azure datalake\n",
    "#%fs mounts\n",
    "\n",
    "#Desmontar el Contenedor especifico\n",
    "#dbutils.fs.unmount(\"/mnt/datalakemlopsd4m/demo/\")\n",
    "\n",
    "#Ver cuales son los archivos que estan en el Directorio\n",
    "#display(dbutils.fs.ls(\"/mnt/datalakemlopsd4m/raw/marcobre/turno1/\"))\n",
    "\n",
    "#Revisar el el bd cargado apartir de spark\n",
    "#display(spark.read.csv(\"/mnt/datalakemlopsd4m/raw/marcobre/turno1/datos_turno1_vf.csv\", header=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46303a45-57ee-43ad-8c08-69a844d9f6e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# **2. Comprensión de los Datos(EDA) y Preprocesamiento de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a3f53bc-757a-4ae2-8312-8356c8a65fd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### *2.1 Cargamos los datos desde el Storage (RAW) para realizar la comprension y preparacion de datos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a04d37-36f4-422c-8135-36e3296503a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Cargar el archivo CSV en un DataFrame de Spark\n",
    "datos_turno1 = spark.read.csv(\"/mnt/datalakemlopsd4m/raw/marcobre/turno1/datos_turno1_vf.csv\", header=True)\n",
    "datos_turno2 = spark.read.csv(\"/mnt/datalakemlopsd4m/raw/marcobre/turno1/datos_turno1_vf.csv\", header=True)\n",
    "\n",
    "# Convertir DataFrame de Spark a DataFrame de Pandas\n",
    "df_turno1 = datos_turno1.toPandas()\n",
    "df_turno2 = datos_turno2.toPandas()\n",
    "\n",
    "# Agregar columna \"turno\" con valor 1 y 2 al DataFrame df_turno1\n",
    "df_turno1['turno'] = 1\n",
    "df_turno2['turno'] = 2\n",
    "\n",
    "#Consolidar los datos del turno 1 y turno 2\n",
    "datos = pd.concat([df_turno1, df_turno2], ignore_index=True)\n",
    "\n",
    "# Ahora puedes manejar el DataFrame con Pandas\n",
    "# Por ejemplo, puedes usar funciones de Pandas como head(), describe(), etc.\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b3f65de-0217-4dfe-96c9-a60ae3eabc48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Paso Final (el DF final de pandas, debes convertirlo a un DF de SPARK, para que los datos limipios sean reflejados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "660edc74-a4b7-441d-aa58-83e78567e5e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Convertimos el df-pandas a un df-spark, para poder guardarlo en el Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d485f6cc-906f-4b31-b1d2-94a2c6feb9d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Guardar luego de convertirlo a un DataFrame de Spark\n",
    "spark_datos = spark.createDataFrame(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96e16c61-ab16-465f-ada8-1725702fc57b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Convertimos el tipo de datos Void a String, para poder guardar en un archivo CSV (Data Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63434025-e3ca-4a46-a825-14cecb4e3205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Lista de todas las columnas\n",
    "columnas = spark_datos.columns\n",
    "\n",
    "# Convertir valores 'void' a cadena vacía ('') en todas las columnas que contienen 'void'\n",
    "for columna in columnas:\n",
    "    spark_datos = spark_datos.withColumn(\n",
    "        columna,\n",
    "        when(col(columna) == 'void', '').otherwise(col(columna))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a63518f-e148-4ad3-8b3d-257d2a26fcfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Guardamos el df-spark en la ruta del Data Storage de Microsoft Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a240571-204e-49d1-b958-087460983bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ruta donde quieres guardar el archivo CSV en tu Azure Data Lake Storage\n",
    "ruta_guardado = \"/mnt/datalakemlopsd4m/raw/marcobre/datosraw/datostotalmarcobre.csv\"\n",
    "\n",
    "# Guardar el DataFrame en formato CSV en la ruta especificada\n",
    "spark_datos.write.csv(ruta_guardado, header=True, mode=\"overwrite\")\n",
    "#spark_datos.repartition(1).write.csv(ruta_guardado, header=True, mode=\"overwrite\") #Sirve para guardar solo en 1 partition csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d7b384-0bb8-4640-8ff6-cb22c9194f60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. Cargamos el CSV(Carpeta Spark) del Data Storage, para verificar que todo este OK (Tambien se convierte de dfSpark a dfPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144d0b18-d4b7-49cb-921c-3b586261cd3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Lee el archivo CSV en un DataFrame de Spark\n",
    "#ruta_carpeta_csv = \"/mnt/datalakemlopsd4m/raw/marcobre/datosraw/datostotalmarcobre.csv\"\n",
    "ruta_carpeta_csv = \"/mnt/datalakemlopsd4m/raw/proyectopases_raw/fuentedatos_c4m/operacion_marcobre/datos_raw_marcobre_2024_04_08.csv\"\n",
    "df_spark = spark.read.option(\"header\", \"true\").csv(ruta_carpeta_csv)\n",
    "\n",
    "# Muestra los primeros registros del DataFrame de Spark Convertido a un DataFrame Pandas\n",
    "df_pandas = df_spark.toPandas()\n",
    "df_pandas.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ebb7ed-1a05-4865-b240-1616023c74d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a70325-0829-4917-8cda-6b9aea80e684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos = df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f71e07-e80c-47cd-a0e5-295ed586470b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Configuramos pandas para que podamos vizualizar todas las columnas y filas la estadistica descriptiva de todas las variables\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "#Configuramos pandas para que lanze valores con una precision de hasta 6 decimales\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdc01c6-e95a-4cab-82b2-dda8e1bb0653",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3.Tratamiento de valores Nulos\n",
    "# 3.1 Supongamos que tienes un DataFrame llamado datos\n",
    "valores_nulos = datos.isnull().sum()\n",
    "valores_nulos_ordenados = valores_nulos.sort_values(ascending=False)\n",
    "porcentaje_nulos = (valores_nulos_ordenados / len(datos)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047eceea-e9bb-48d3-81b1-04ff59e722e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columnas_a_eliminar = porcentaje_nulos[porcentaje_nulos > 80].index\n",
    "datos = datos.drop(columnas_a_eliminar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7f082c-22b8-44d8-8594-bdd8ff0e0e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos = datos.drop(['tipoubicacionsupervisor_camion','tipoubicacionsupervisor_pala','id_cargadescarga_pases','dumpreal','loadreal', 'rownum_global'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd816609-0f33-4c7b-b43d-b6c6866f7840",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f2a4ca-3a66-43fd-a504-8cc921bf7f8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcula la moda de 'has_block_pases'\n",
    "moda_has_block_pases = datos['has_block_pases'].mode()[0]\n",
    "\n",
    "# Completa los valores nulos con la moda en la columna 'has_block_pases'\n",
    "datos['has_block_pases'].fillna(moda_has_block_pases, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a407062-dc8c-4c18-a668-fc4d41b8b804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "moda_tipodescargaidentifier = datos['tipodescargaidentifier'].mode()[0]\n",
    "\n",
    "# Completa los valores nulos con la moda en la columna 'tipodescargaidentifier'\n",
    "datos['tipodescargaidentifier'].fillna(moda_tipodescargaidentifier, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ce7480a-7429-4a33-b311-a8c062528d68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Completa los datos Nulos por 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9d028c-8149-43be-8734-a0158e08cc3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos = datos.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cb02fb2-cae7-4df3-96a3-eb5ae1564865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos = datos.drop_duplicates()\n",
    "datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3ed6bc-078a-4d68-979c-01f5874ea7af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66106b0d-836e-41c0-b82e-4c77fd3590a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Supongamos que 'datos' es tu DataFrame y 'columnas_fecha' son las columnas que contienen fechas\n",
    "columnas_fecha = ['tiem_llegada_global', 'tiem_esperando', 'tiem_cuadra', 'tiem_cuadrado', 'tiem_carga', 'tiem_acarreo', 'tiem_cola', 'tiem_retro', 'tiem_listo', 'tiem_descarga', 'tiem_viajando', 'tiempo_inicio_carga_carguio', 'tiempo_esperando_carguio', 'previous_esperando_pala', 'tiempo_inicio_cambio_estado_camion', 'tiempo_inicio_cambio_estado_pala']\n",
    "\n",
    "# Convertir todas las fechas al mismo formato\n",
    "for columna in columnas_fecha:\n",
    "    datos[columna] = pd.to_datetime(datos[columna], errors='coerce') #Si hubiese una fecha con  Error, lo reemplaza con NAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a0014e-ac97-43ee-a321-a123a38a1624",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reemplazar los valores nulos con la fecha de la fila anterior más 3 segundos adicionales\n",
    "for columna in columnas_fecha:\n",
    "    mask_nat = datos[columna].isna()\n",
    "    datos[columna].loc[mask_nat] = datos[columna].fillna(method='ffill') + pd.to_timedelta(3, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a563de-ce3b-4044-8e9d-fc503cc1036a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Formatear las fechas en el formato deseado\n",
    "formato_deseado = \"%Y-%m-%d %H:%M:%S.%f%z\"  # Formato deseado\n",
    "for columna in columnas_fecha:\n",
    "    datos[columna] = datos[columna].dt.strftime(formato_deseado)\n",
    "    datos[columna] = pd.to_datetime(datos[columna])\n",
    "    datos[columna] = datos[columna] + pd.to_timedelta(999, unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abfed88-2fab-4083-a57a-574361c657e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supongamos que 'datos' es tu DataFrame\n",
    "\n",
    "# Diccionario para especificar los tipos de datos deseados para cada columna\n",
    "tipos_de_datos = {\n",
    "    'id_ciclo_acarreo': 'int64','id_cargadescarga': 'int64','id_palas': 'int64','id_equipo_camion': 'int64','id_ciclo_carguio': 'float64',\n",
    "    'id_equipo_carguio': 'float64','id_trabajador_pala': 'float64','id_guardia_realiza_carga_al_camion': 'float64','id_locacion': 'float64',\n",
    "    'id_poligono_se_obtiene_material': 'float64','tiempo_ready_cargando_pala': 'float64','tiempo_ready_esperando_pala': 'float64',\n",
    "    'cantidad_equipos_espera_al_termino_carga_pala': 'float64','id_estados_camion': 'int64','id_equipo_table_estados_camion': 'int64',\n",
    "    'id_detal_estado_camion': 'int64','tiempo_estimado_duracion_estado_camion': 'int64','en_campo_o_taller_mantenimiento_camion': 'int64',\n",
    "    'id_tipo_estad_camion': 'int64','id_estados_pala': 'float64','id_equipo_table_estados_pala': 'float64','id_detal_estado_pala': 'float64',\n",
    "    'tiempo_estimado_duracion_estado_pala': 'float64','en_campo_o_taller_mantenimiento_pala': 'float64','id_tipo_estad_pala': 'float64',\n",
    "    'id_descarga': 'int64','id_factor': 'int64','id_poligono': 'float64','tiempo_ready_llegada_esperando': 'float64',\n",
    "    'tiempo_ready_esperando_cuadra': 'float64','tiempo_ready_cuadra_cuadrado': 'float64', 'tiempo_ready_cuadrado_cargado': 'float64',\n",
    "    'tiempo_ready_carga_acarreo': 'float64','tiempo_ready_acarreo_cola': 'float64','tiempo_ready_cola_retro': 'float64',\n",
    "    'tiempo_ready_retro_listo': 'float64','tiempo_ready_listo_descarga': 'float64','tiempo_ready_descarga_viajandovacio': 'float64',\n",
    "    'id_trabajador_camion': 'int64','id_palanext': 'int64','tonelaje': 'float64','tonelajevims': 'float64','yn_estado': 'bool',\n",
    "    'id_guardia_hizocarga': 'int64','id_guardia_hizodescarga': 'int64','id_zona_aplicafactor': 'int64','id_zona_pertenece_poligono': 'float64',\n",
    "    'factor': 'int64','toneladas_secas': 'float64','productividad_operativa_acarreo_tn_h': 'float64','productividad_operativa_carguio_tn_h': 'float64','efhcargado': 'float64','efhvacio': 'float64','distrealcargado': 'float64','distrealvacio': 'float64','coorxdesc': 'float64',\n",
    "    'coorydesc': 'float64','coorzdesc': 'float64','tipodescargaidentifier': 'float64','tonelajevvanterior': 'int64','tonelajevvposterior': 'float64','velocidadvimscargado': 'float64','velocidadvimsvacio': 'float64','velocidadgpscargado': 'float64','velocidadgpsvacio': 'float64','tonelajevimsretain': 'float64', 'nivelcombuscargado': 'float64','nivelcombusdescargado':'float64',\n",
    "    'volumen': 'float64','aplicafactor_vol': 'bool','coorzniveldescarga': 'float64','efh_factor_loaded': 'float64','efh_factor_empty':'float64',\n",
    "    'id_secundario': 'int64','id_principal': 'int64','capacidad_vol_equipo': 'float64','capacidad_pes_equipo': 'float64',\n",
    "    'capacidadtanque_equipo': 'int64','peso_bruto_equipo': 'float64','ishp_equipo': 'bool','ancho_equipo': 'int64','largo_equipo': 'int64',\n",
    "    'numeroejes_equipo': 'int64','id_turnos_turnocarga': 'int64','horaini_turnocarga': 'int64','horafin_turnocarga': 'int64',\n",
    "    'id_turnos_turnodescarga': 'int64','horaini_turnodescarga': 'int64','horafin_turnodescarga': 'int64',\n",
    "    'id_zona_encuentra_descarga': 'int64','id_nodo_carga': 'float64','id_nodo_descarga': 'int64',\n",
    "    'elevacion_descarga': 'int64','nivel_elevacion_locacion_mts': 'float64','radio_locacion': 'float64','id_material': 'float64',\n",
    "    'elevacion_poligono_mts': 'float64','densidad_poligono': 'float64','tonelaje_inicial_poligono': 'float64','id_pases': 'float64',\n",
    "    'id_palas_pases': 'float64','angulo_giro_promedio_pases': 'float64','has_block_pases': 'bool'}\n",
    "\n",
    "# Convertir las columnas al tipo de dato correspondiente\n",
    "for columna, tipo in tipos_de_datos.items():\n",
    "    datos[columna] = datos[columna].astype(tipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03913603-dd8a-4ec8-8d89-87c1d3a5ec48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#for columna, tipo in tipos_de_datos.items():\n",
    "#    if columna in datos.columns:\n",
    "#        datos[columna] = datos[columna].astype(tipo)\n",
    "#    else:\n",
    "#        print(f\"La columna '{columna}' no está presente en el DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eb0a867-c69a-4d33-94a2-2cfbca4716fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Transformacion de datos que tienen medidas distintas (ejemplo: cm, metros, km, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c1054b-41de-4e7d-93d3-f6a8dc73e991",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pasar tonelajevims a toneladas\n",
    "datos['tonelajevims'] = datos['tonelajevims'] / 10\n",
    "\n",
    "#pasar efhcargado de centimetros a metros\n",
    "datos['efhcargado'] = datos['efhcargado'] / 100\n",
    "\n",
    "#pasar efhvacio de centimetros a metros\n",
    "datos['efhvacio'] = datos['efhvacio'] / 100\n",
    "\n",
    "#pasar distrealcargado de centimetros a metros\n",
    "datos['distrealcargado'] = datos['distrealcargado'] / 100\n",
    "\n",
    "#pasar distrealvacio de centimetros a metros\n",
    "datos['distrealvacio'] = datos['distrealvacio'] / 100\n",
    "\n",
    "#pasar coorxdesc de centimetros a Km\n",
    "datos['coorxdesc'] = datos['coorxdesc'] / 100000\n",
    "\n",
    "#pasar coorydesc de centimetros a Km\n",
    "datos['coorydesc'] = datos['coorydesc'] / 100000\n",
    "\n",
    "#pasar coorzdesc de centimetros a Km\n",
    "datos['coorzdesc'] = datos['coorzdesc'] / 100000\n",
    "\n",
    "#pasar velocidadvimscargado a  Km/hr\n",
    "datos['velocidadvimscargado'] = datos['velocidadvimscargado'] / 1000\n",
    "\n",
    "#pasar velocidadvimsvacio a  Km/hr\n",
    "datos['velocidadvimsvacio'] = datos['velocidadvimsvacio'] / 1000\n",
    "\n",
    "#pasar velocidadvimsvacio a  Km/hr\n",
    "datos['velocidadgpscargado'] = datos['velocidadgpscargado'] / 1000\n",
    "\n",
    "#pasar velocidadvimsvacio a  Km/hr\n",
    "datos['velocidadgpsvacio'] = datos['velocidadgpsvacio'] / 1000\n",
    "\n",
    "#pasar tonelajevimsretain a  toneladas\n",
    "datos['tonelajevimsretain'] = datos['tonelajevimsretain'] / 10\n",
    "\n",
    "#pasar coorzniveldescarga de centimetros a metros\n",
    "datos['coorzniveldescarga'] = datos['coorzniveldescarga'] / 100\n",
    "\n",
    "#pasar ancho_equipo de centimetros a metros\n",
    "datos['ancho_equipo'] = datos['ancho_equipo'] / 100\n",
    "\n",
    "#pasar largo_equipo de centimetros a metros\n",
    "datos['largo_equipo'] = datos['largo_equipo'] / 100\n",
    "\n",
    "#pasar tonelajevvanterior a toneladas\n",
    "datos['tonelajevvanterior'] = datos['tonelajevvanterior'] / 10\n",
    "\n",
    "#pasar tonelajevvposterior a toneladas\n",
    "datos['tonelajevvposterior'] = datos['tonelajevvposterior'] / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e10661-8225-4f8a-b1e0-336b99531bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6dee24-097e-46c0-9d18-a58aba35af17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046ddbcc-50cb-441f-8282-bd349c452a96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1ba88e-b772-45bd-8d8e-366632bce5df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'].mode().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61e7a2c-f428-46b1-add7-bd4f3ebd1873",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reemplazar '0' por la moda\n",
    "datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'] = datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'].replace(0,  datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'].mode().iloc[0])\n",
    "\n",
    "# Reemplazar 'True' por True y 'False' por False , Paso crucial para antes de Convertir a DATOS BOOLEANO\n",
    "datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'] = datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'].replace({'True': True, 'False': False})\n",
    "\n",
    "# Convertir la columna a tipo de datos booleano\n",
    "datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'] = datos['termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de811fc7-df46-45e1-861a-9f1e3f2441a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reemplazar '0' por la moda\n",
    "datos['cambio_estado_operatividad_carguio'] =datos['cambio_estado_operatividad_carguio'].replace(0, datos['cambio_estado_operatividad_carguio'].mode().iloc[0])\n",
    "\n",
    "# Reemplazar 'True' por True y 'False' por False , Paso crucial para antes de Convertir a DATOS BOOLEANO\n",
    "datos['cambio_estado_operatividad_carguio'] = datos['cambio_estado_operatividad_carguio'].replace({'True': True, 'False': False})\n",
    "\n",
    "# Convertir la columna a tipo de datos booleano\n",
    "datos['cambio_estado_operatividad_carguio'] = datos['cambio_estado_operatividad_carguio'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f89954a-3e69-4950-981c-5af33e2348ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificar si la variable es True\n",
    "if datos['cambio_estado_operatividad_carguio'].iloc[16] == True:  #  datos['cambio_estado_operatividad_carguio'].iloc[32] == True\n",
    "    print(\"La variable es True\")\n",
    "else:\n",
    "    print(\"La variable no es True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c35bdf0-ad41-43e5-aa40-defb0a1bc53f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcecf68a-a8a6-4850-b77f-a461e0f6c49e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define un diccionario con los nuevos nombres de las columnas solo para algunas columnas\n",
    "nuevos_nombres = {'id_cargadescarga' : 'id_cargadescarga_ciclo',\n",
    "    'termino_carga_equipo_en_espera_cuadrado_cuadrandose_carguio' : 'al_termino_cargar_en_espera_cuadrado_cuadrandose',\n",
    "    'id_descarga' : 'id_zona_hace_descarga',  \n",
    "    'tiem_llegada_global': 'tiempo_llegada_camion', 'tiem_esperando': 'tiempo_esperando_camion_en_locacion', \n",
    "    'tiem_cuadra': 'tiempo_cuadra_camion','tiem_cuadrado': 'tiempo_cuadrado_camion' , 'tiem_carga' : 'tiempo_cargar_al_camion', \n",
    "    'tiem_acarreo' : 'tiempo_acarreo_camion', 'tiem_cola': 'tiempo_cola_camion_en_zonadescarga','tiem_retro': 'tiempo_retroceso_para_descargar',\n",
    "    'tiem_listo' : 'tiempo_listo_para_descargar',  'tiem_descarga': 'tiempo_descarga_camion', 'tiem_viajando': 'tiempo_viajando_vacio_locacion', \n",
    "    'tonelaje':'tonelaje_nominal', 'tonelajevims':'tonelaje_segun_computadora', 'yn_estado': 'cambios_estado_en_ciclo',\n",
    "    'distrealcargado': 'distancia_recorrida_camioncargado_km_gps_mts', 'distrealvacio': 'distancia_recorrida_camionvacio_km_gps_mts' ,\n",
    "    'coorxdesc': 'coordenada_x_descarga_km', 'coorydesc': 'coordenada_y_descarga_km' , 'coorzdesc': 'coordenada_z_descarga_km',\n",
    "    'tipodescargaidentifier': 'tipo_descarga_efectuado', \n",
    "    'tonelajevvanterior': 'tonelaje_camion_viajevacio_cicloanterior_vims', 'tonelajevvposterior': 'tonelaje_camion_viajevacio_cicloactual_vims',\n",
    "    'velocidadvimscargado': 'promedio_velocidad_camioncargado_km/hr_compu', \n",
    "    'velocidadvimsvacio': 'promedio_velocidad_camionvacio_km/hr_compu',\n",
    "    'velocidadgpscargado':'promedio_velocidad_camioncargado_km/hr_gps', 'velocidadgpsvacio': 'promedio_velocidad_camionvacio_km/hr_gps', \n",
    "    'tonelajevimsretain': 'tonelaje_camion_antes_cargaestabilizada', 'nivelcombuscargado': 'porcentaje_combustible_camioncargando', \n",
    "    'nivelcombusdescargado':'porcentaje_combustible_camiondescargando', 'volumen': 'volumen_nominal', 'aplicafactor_vol': 'aplica_factor_volumen_o_tonelaje',\n",
    "    'coorzniveldescarga': 'nivel_descarga_metros', 'nombre_equipo':'nombre_equipo_acarreo', \n",
    "    'id_secundario':'id_flota_secundaria', 'flota_secundaria':'nombre_flota_secundaria', 'id_principal': 'id_flota_principal', 'flota_principal':'nombre_flota_principal',\n",
    "    'capacidad_vol_equipo': 'capacidad_en_volumen_equipo_acarreo_m3', 'capacidad_pes_equipo':'capacidad_en_peso_equipo_acarreo', 'capacidadtanque_equipo': 'capacidadtanque_equipoacarreo_galones',\n",
    "    'peso_bruto_equipo':'peso_bruto_equipo_acarreo', 'ishp_equipo':'si_no_equipo_altaprecision', 'ancho_equipo':'ancho_equipo_metros', 'largo_equipo':'largo_equipo_metros',\n",
    "    'elevacion_descarga':'nivel_elevacion_descarga_metros', 'nombre_descarga':'nombre_zona_descarga', \n",
    "    'nombre_carga_locacion':'nombre_locacion_carga', 'nivel_elevacion_locacion_mts':'nivel_elevacion_locacion_carga_metros', 'radio_locacion':'radio_locacion_metros',\n",
    "    'ids_poligonos_en_locacion':'ids_poligonos_en_locacion_carga', 'id_material': 'id_material_dominante_en_poligono', \n",
    "    'elevacion_poligono_mts':'elevacion_poligono_metros', 'ley_in':'lista_leyes', 'densidad_poligono':'densidad_inicial_poligono_creado_tn/m3',\n",
    "    'capacidad_vol' : 'capacidad_en_volumen_equipo_carguio_m3', 'capacidad_pes':'capacidad_en_peso_equipo_carguio', 'capacidadtanque': 'capacidadtanque_equipocarguio_galones',\n",
    "    'radiohexagonocuchara' : 'radiohexagonocuchara_equipocarguio', 'id_tablegen' : 'id_guardia_acarreocarga', 'nombre_tablegen' : 'nombre_guardia_acarreocarga', \n",
    "    'id_guardiadescarga': 'id_guardia_acarreodescarga', 'nombre_guardiadescarga':'nombre_guardia_acarreodescarga',\n",
    "    'id':'id_guardia_carguio', 'nombre': 'nombre_guardia_carguio', 'id_locacion' : 'id_locacion_hace_carga','tonelaje_inicial_poligono': 'tonelaje_inicial_poligono_creado',  'efhvacio':'efhvacio_mts', 'efhcargado':'efhcargado_mts', \n",
    "}\n",
    "# Renombra las columnas del DataFrame\n",
    "datos = datos.rename(columns=nuevos_nombres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5efbccac-ff02-46df-84e9-a71f1e4c4a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 11. Agregamos Nuevas variables calculadas\n",
    "# Agregamos la variable 'porcentaje_eficiencia_toneladas_movidas_acarreo'\n",
    "datos['porcentaje_eficiencia_toneladas_movidas_acarreo'] = (datos['tonelaje_segun_computadora'] / datos['tonelaje_nominal']) * 100\n",
    "\n",
    "#Agregamos la variable 'altura_elevacion'\n",
    "datos['altura_elevacion'] = abs(datos['nivel_elevacion_descarga_metros'] - datos['nivel_elevacion_locacion_carga_metros'] )\n",
    "\n",
    "# Agregamos la variable 'factor_perfil_rutavacio_mts'\n",
    "datos['factor_perfil_rutavacio'] = np.where(datos['distancia_recorrida_camionvacio_km_gps_mts'] != 0,\n",
    "                                            datos['efhvacio_mts'] / datos['distancia_recorrida_camionvacio_km_gps_mts'],\n",
    "                                            0)\n",
    "\n",
    "# Agregamos la variable 'factor_perfil_rutacargado_mts'\n",
    "datos['factor_perfil_rutacargado'] = np.where(datos['distancia_recorrida_camioncargado_km_gps_mts'] != 0,\n",
    "                                              datos['efhcargado_mts'] / datos['distancia_recorrida_camioncargado_km_gps_mts'],\n",
    "                                              0)\n",
    "\n",
    "# Agregamos la variable calculada \"numero_pases_carguio\" basado en la columna 'coord_x_pases'\n",
    "datos['numero_pases_carguio'] = datos['coord_x_pases'].apply(lambda x: len(eval(x)) if isinstance(x, str) and '[' in x else x if isinstance(x, int) else 0)\n",
    "\n",
    "#Agregamos la variable Galones_diponible_camioncargando\n",
    "datos['Galones_disponibles_camioncargando'] = (datos['porcentaje_combustible_camioncargando']/100) * datos['capacidadtanque_equipoacarreo_galones']\n",
    "#datos['demanda_galones_camioncargando'] = (datos['porcentaje_combustible_camioncargando']/100) * datos['capacidadtanque_equipoacarreo_galones']\n",
    "\n",
    "#Agregar la variable Galones_diponible_camiondescargando \n",
    "datos['Galones_disponibles_camiondescargando'] = (datos['porcentaje_combustible_camiondescargando']/100) * datos['capacidadtanque_equipoacarreo_galones']\n",
    "\n",
    "#Agregar la variable Galones_consumidos_entre_cargando_descargando \n",
    "datos['Galones_consumidos_entre_cargando_descargando_acarreo'] = datos['Galones_disponibles_camioncargando'] - datos['Galones_disponibles_camiondescargando']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6355b419-e769-4033-af29-0c5857c0aacf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **3. Guardar los Datos procesados(spark df) apartir del storage RAW, en una tabla DELTA (hacia storage PROCESSED)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc9a9da4-51b1-4775-9e2c-f47df83802d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Montamos el almacenamiento Processed de Data Lake donde gaurdaremos estos datos si no esta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcd1c53-fbd2-4d67-b79f-d1d574a0202d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Call file system utlity mount to mount the storage\n",
    "# dbutils.fs.mount(\n",
    "#   source = \"abfss://processed@datalakemlopsd4m.dfs.core.windows.net/\",\n",
    "#   mount_point = \"/mnt/datalakemlopsd4m/processed/\",\n",
    "#   extra_configs = configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8712499-a7b5-4d78-ad41-d7067b3be738",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creamos la BD donde almacenaremos las Tables DELTA (Verificar si ya esta creada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d86a48d-6907-4d50-bf21-e5ede0ee511f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear la base de datos si no existe en el almacenamiento de PROCESSED\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS proyectopases_processed LOCATION '/mnt/datalakemlopsd4m/processed/proyectopases_processed/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd1b7af-cab1-4bc3-bf65-fb32cbc197c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listar todas las bases de datos\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbab3678-a688-4e8a-b29d-9c94f57360cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Guardamos el df preprocesado en la tabla DELTA (datos_turno1_deltavf) dentro de la BD (processed_db)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d79b115-c26b-4071-ad82-018d98b97fe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_datos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"processed_db.datos_processed_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf9b825-51f6-45b4-b051-cf2816c697f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Verificamos que se creo la Tabla DELTA en la BD adecuada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e127c41-0e43-4dc4-a263-880f59da48bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listar todas las bases de datos\n",
    "#spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Listar las tablas en una base de datos específica (por ejemplo, processed_db)\n",
    "spark.sql(\"SHOW TABLES IN processed_db\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.preprocessing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
