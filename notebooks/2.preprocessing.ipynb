{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95c7541f-3028-4e06-abf0-a15f656054ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Mount Azure Data Lake using Service Principal\n",
    "#### Steps to follow\n",
    "1. Get client_id, tenant_id and client_secret from key vault\n",
    "2. Set Spark Config with App/ Client Id, Directory/ Tenant Id & Secret\n",
    "3. Call file system utlity mount to mount the storage\n",
    "4. Explore other file system utlities related to mount (list all mounts, unmount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d98ed6-d9d0-49d8-a427-6fdc5bee19f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Obtenemos las credenciales necesarias para montar un almacenamiento en Microsoft Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d7ac18-184c-44e2-b537-fc39818d91b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Get client_id, tenant_id and client_secret from key vault\n",
    "client_id = dbutils.secrets.get(scope = 'mlops-scope', key = 'mlops-app-client-id')\n",
    "tenant_id = dbutils.secrets.get(scope = 'mlops-scope', key = 'mlops-app-tenant-id')\n",
    "client_secret = dbutils.secrets.get(scope = 'mlops-scope', key = 'mlops-app-client-secret')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d02257b-5e44-4769-b514-da20ae34d99e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Asignamos las variables necesariamos para montar algun Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08529587-4086-4ccc-9c83-402e9c0601e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set Spark Config with App/ Client Id, Directory/ Tenant Id & Secret\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "          \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "          \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "          \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46fec388-7216-44bb-8405-95650be70164",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Montamos el almacenamiento de Data Lake que deseamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b57230-ec23-41d7-bba6-f3370b056215",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Call file system utlity mount to mount the storage\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://raw@datalakemlopsd4m.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/datalakemlopsd4m/raw/\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdd0054-ef8b-4d97-a7da-e92d8dbf199f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Verificar que contenedores estan montados en el azure datalake\n",
    "#%fs mounts\n",
    "\n",
    "#Desmontar el Contenedor especifico\n",
    "#dbutils.fs.unmount(\"/mnt/datalakemlopsd4m/demo/\")\n",
    "\n",
    "#Ver cuales son los archivos que estan en el Directorio\n",
    "#display(dbutils.fs.ls(\"/mnt/datalakemlopsd4m/raw/marcobre/turno1/\"))\n",
    "\n",
    "#Revisar el el bd cargado apartir de spark\n",
    "#display(spark.read.csv(\"/mnt/datalakemlopsd4m/raw/marcobre/turno1/datos_turno1_vf.csv\", header=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46303a45-57ee-43ad-8c08-69a844d9f6e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# **2. Comprensión de los Datos(EDA) y Preprocesamiento de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a3f53bc-757a-4ae2-8312-8356c8a65fd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### *2.1 Cargamos los datos desde el Storage (RAW) para realizar la comprension y preparacion de datos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a04d37-36f4-422c-8135-36e3296503a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Cargar el archivo CSV en un DataFrame de Spark\n",
    "datos_turno1 = spark.read.csv(\"/mnt/datalakemlopsd4m/raw/marcobre/turno1/datos_turno1_vf.csv\", header=True)\n",
    "datos_turno2 = spark.read.csv(\"/mnt/datalakemlopsd4m/raw/marcobre/turno1/datos_turno1_vf.csv\", header=True)\n",
    "\n",
    "# Convertir DataFrame de Spark a DataFrame de Pandas\n",
    "df_turno1 = datos_turno1.toPandas()\n",
    "df_turno2 = datos_turno2.toPandas()\n",
    "\n",
    "# Agregar columna \"turno\" con valor 1 y 2 al DataFrame df_turno1\n",
    "df_turno1['turno'] = 1\n",
    "df_turno2['turno'] = 2\n",
    "\n",
    "#Consolidar los datos del turno 1 y turno 2\n",
    "datos = pd.concat([df_turno1, df_turno2], ignore_index=True)\n",
    "\n",
    "# Ahora puedes manejar el DataFrame con Pandas\n",
    "# Por ejemplo, puedes usar funciones de Pandas como head(), describe(), etc.\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b3f65de-0217-4dfe-96c9-a60ae3eabc48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Paso Final (el DF final de pandas, debes convertirlo a un DF de SPARK, para que los datos limipios sean reflejados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "660edc74-a4b7-441d-aa58-83e78567e5e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Convertimos el df-pandas a un df-spark, para poder guardarlo en el Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d485f6cc-906f-4b31-b1d2-94a2c6feb9d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Guardar luego de convertirlo a un DataFrame de Spark\n",
    "spark_datos = spark.createDataFrame(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96e16c61-ab16-465f-ada8-1725702fc57b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Convertimos el tipo de datos Void a String, para poder guardar en un archivo CSV (Data Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63434025-e3ca-4a46-a825-14cecb4e3205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Lista de todas las columnas\n",
    "columnas = spark_datos.columns\n",
    "\n",
    "# Convertir valores 'void' a cadena vacía ('') en todas las columnas que contienen 'void'\n",
    "for columna in columnas:\n",
    "    spark_datos = spark_datos.withColumn(\n",
    "        columna,\n",
    "        when(col(columna) == 'void', '').otherwise(col(columna))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a63518f-e148-4ad3-8b3d-257d2a26fcfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Guardamos el df-spark en la ruta del Data Storage de Microsoft Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a240571-204e-49d1-b958-087460983bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ruta donde quieres guardar el archivo CSV en tu Azure Data Lake Storage\n",
    "ruta_guardado = \"/mnt/datalakemlopsd4m/raw/marcobre/datosraw/datostotalmarcobre.csv\"\n",
    "\n",
    "# Guardar el DataFrame en formato CSV en la ruta especificada\n",
    "spark_datos.write.csv(ruta_guardado, header=True, mode=\"overwrite\")\n",
    "#spark_datos.repartition(1).write.csv(ruta_guardado, header=True, mode=\"overwrite\") #Sirve para guardar solo en 1 partition csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d7b384-0bb8-4640-8ff6-cb22c9194f60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. Cargamos el CSV(Carpeta Spark) del Data Storage, para verificar que todo este OK (Tambien se convierte de dfSpark a dfPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144d0b18-d4b7-49cb-921c-3b586261cd3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lee el archivo CSV en un DataFrame de Spark\n",
    "ruta_carpeta_csv = \"/mnt/datalakemlopsd4m/raw/marcobre/datosraw/datostotalmarcobre.csv\"\n",
    "df_spark = spark.read.option(\"header\", \"true\").csv(ruta_carpeta_csv)\n",
    "\n",
    "# Muestra los primeros registros del DataFrame de Spark Convertido a un DataFrame Pandas\n",
    "df_pandas = df_spark.toPandas()\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ebb7ed-1a05-4865-b240-1616023c74d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6355b419-e769-4033-af29-0c5857c0aacf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **3. Guardar los Datos procesados(spark df) apartir del storage RAW, en una tabla DELTA (hacia storage PROCESSED)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc9a9da4-51b1-4775-9e2c-f47df83802d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Montamos el almacenamiento de Data Lake donde gaurdaremos estos datos si no esta (Processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcd1c53-fbd2-4d67-b79f-d1d574a0202d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Call file system utlity mount to mount the storage\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://processed@datalakemlopsd4m.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/datalakemlopsd4m/processed/\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8712499-a7b5-4d78-ad41-d7067b3be738",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creamos la BD donde almacenaremos las Tables DELTA (Verificar si ya esta creada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd1b7af-cab1-4bc3-bf65-fb32cbc197c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listar todas las bases de datos\n",
    "#spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d86a48d-6907-4d50-bf21-e5ede0ee511f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear la base de datos si no existe en el almacenamiento de PROCESSED\n",
    "#spark.sql(\"CREATE DATABASE IF NOT EXISTS processed_db LOCATION '/mnt/datalakemlopsd4m/processed/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbab3678-a688-4e8a-b29d-9c94f57360cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Guardamos el df preprocesado en la tabla DELTA (datos_turno1_deltavf) dentro de la BD (processed_db)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d79b115-c26b-4071-ad82-018d98b97fe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_datos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"processed_db.datos_processed_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf9b825-51f6-45b4-b051-cf2816c697f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Verificamos que se creo la Tabla DELTA en la BD adecuada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e127c41-0e43-4dc4-a263-880f59da48bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listar todas las bases de datos\n",
    "#spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Listar las tablas en una base de datos específica (por ejemplo, processed_db)\n",
    "spark.sql(\"SHOW TABLES IN processed_db\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 712926803047545,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.preprocessing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
