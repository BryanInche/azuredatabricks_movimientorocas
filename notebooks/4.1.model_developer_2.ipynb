{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81055a5a-6333-482a-a715-3b821e6cbb3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "\n",
    "#gf\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.tensorflow\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#import streamlit as st\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "#Librerias para redes neuronales(secuencial)\n",
    "from tensorflow.keras.models import Sequential\n",
    "#Capa completamente conectada\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "#Optimizador\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configurar pandas para mostrar todas las filas y columnas\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca31b2b8-d5e1-40f9-9830-e85c84caeee2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. Leer datos para crear los Modelos IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae81f2d-e62a-4e11-88a6-434608123224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/presentation/proyectopases_presentation/datos_presentation_tabladelta_bi\")\n",
    "datos = df_delta.toPandas()\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d22fb635-03f6-4633-a070-077f5e257677",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2 Metodo 2 -  Usar un bucle para realizar el codigo de One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "955cee58-151d-4fb2-8be8-93bb374f9f11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "VARIABLE nombre_equipo_carguio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f270c79c-c994-4983-a990-7d8ca3523425",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "categorias = datos['nombre_equipo_carguio'].unique()\n",
    "\n",
    "# Crear un DataFrame vacío con columnas para cada categoría, Diccionario one_hot_dict : categoria: clave es una categoría única de #nombre_equipo_carguio, y el [0]*len(datos) : valor asociado a cada clave es una lista de ceros de la misma longitud que el DataFrame \n",
    "one_hot_dict = {categoria: [0]*len(datos) for categoria in categorias}\n",
    "\n",
    "# Llenar el diccionario con 1s en las posiciones correspondientes\n",
    "# idx es el índice de la fila, val es el valor de nombre_equipo_carguio en esa fila\n",
    "# Si idx=0 y val='CF02', entonces one_hot_dict['CF02'][0] = 1\n",
    "for idx, val in datos['nombre_equipo_carguio'].items():\n",
    "    one_hot_dict[val][idx] = 1\n",
    "\n",
    "# Convertir el diccionario a un DataFrame\n",
    "one_hot_df = pd.DataFrame(one_hot_dict)\n",
    "\n",
    "#Unir el DataFrame original con el codificado\n",
    "datos = pd.concat([datos, one_hot_df], axis=1)\n",
    "\n",
    "# # Mostrar el resultado\n",
    "datos[['nombre_equipo_carguio','CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed807a88-e56a-479a-bcb6-24c148f99118",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "VARIABLE Procedencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175e12de-2e64-48f2-a3ac-f4e97bb215ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener las categorías únicas\n",
    "categorias_pro = datos['Procedencia'].unique()\n",
    "\n",
    "# Crear un DataFrame vacío con columnas para cada categoría, Diccionario one_hot_dict : categoria: clave es una categoría única de #nombre_equipo_carguio, y el [0]*len(datos) : valor asociado a cada clave es una lista de ceros de la misma longitud que el DataFrame \n",
    "one_hot_dict_pro = {categoria: [0]*len(datos) for categoria in categorias_pro}\n",
    "\n",
    "# Llenar el diccionario con 1s en las posiciones correspondientes\n",
    "# idx es el índice de la fila, val es el valor de nombre_equipo_carguio en esa fila\n",
    "# Si idx=0 y val='CF02', entonces one_hot_dict['CF02'][0] = 1\n",
    "for idx, val in datos['Procedencia'].items():\n",
    "    one_hot_dict_pro[val][idx] = 1\n",
    "\n",
    "# Convertir el diccionario a un DataFrame\n",
    "one_hot_dict_pro_df = pd.DataFrame(one_hot_dict_pro)\n",
    "\n",
    "#Unir el DataFrame original con el codificado\n",
    "datos = pd.concat([datos, one_hot_dict_pro_df], axis=1)\n",
    "\n",
    "# # Mostrar el resultado\n",
    "datos[['Procedencia','TAJ', 'SUL', 'OXI', 'NoDefinido']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d787bf5c-8159-44a8-a523-8ff451e892c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. Hacemos un tratamiento de variable Boleana para convertir a Numerica por el Metodo del Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c27740-169e-4cc7-8ab5-28ecf37abbf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Variable al_termino_cargar_en_espera_cuadrado_cuadrandose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f316c5-cc98-4d2f-b5b1-6ae102875199",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Crear un diccionario para mapear valores booleanos a numéricos\n",
    "bool_to_num = {True: 1, False: 0}\n",
    "\n",
    "# # Aplicar el diccionario a la columna booleana para convertirla a numérica\n",
    "datos['al_termino_cargar_en_espera_cuadrado_cuadrandose'] = datos['al_termino_cargar_en_espera_cuadrado_cuadrandose'].map(bool_to_num)\n",
    "\n",
    "# Mostrar el resultado\n",
    "datos['al_termino_cargar_en_espera_cuadrado_cuadrandose'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "617332a3-aebc-4c21-a9a8-40347b443bea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Variables Seleccionadas antes de entrenar los modelos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ec5b80-adbf-4526-9ba5-891ae442b8d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_subset=datos[['tonelaje_inicial_poligono_creado',\n",
    "#'radiohexagonocuchara_equipocarguio',\n",
    "'capacidad_en_volumen_equipo_carguio_m3',\n",
    "#'capacidad_en_volumen_equipo_acarreo_m3', #varianza 0\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "#'capacidad_en_peso_equipo_acarreo',  # varianza 0\n",
    "'tiempo_estimado_duracion_estado_pala',\n",
    "'radio_locacion_metros',\n",
    "'tiempo_ready_llegada_esperando',\n",
    "#'tonelaje_camion_antes_cargaestabilizada',\n",
    "'tonelaje_segun_computadora',\n",
    "'densidad_inicial_poligono_creado_tn/m3','tiempo_carga','al_termino_cargar_en_espera_cuadrado_cuadrandose',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala','horaini_turnocarga','elevacion_poligono_metros',\n",
    "'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "'TAJ', 'SUL', 'OXI', 'NoDefinido']]\n",
    "datos_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a8a1499-e855-437e-8e83-18665bec8756",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###4. Funcion para Escalar datos, y guardar los valores para escalar en produccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71f98d06-984b-4093-899a-6e5021751bca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4.1 Hallamos los Minimos y Maximos de cada Variable a escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48b8df9-a779-4a27-a1e3-7504e9662007",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecciona las variables específicas\n",
    "variables_especificas = ['tonelaje_inicial_poligono_creado',\n",
    "                         'capacidad_en_volumen_equipo_carguio_m3',\n",
    "                         'capacidad_en_peso_equipo_carguio',\n",
    "                         'tiempo_estimado_duracion_estado_pala',\n",
    "                         'radio_locacion_metros',\n",
    "                         'tiempo_ready_llegada_esperando',\n",
    "                         'tonelaje_segun_computadora',\n",
    "                         'densidad_inicial_poligono_creado_tn/m3','tiempo_carga',\n",
    "                         'cantidad_equipos_espera_al_termino_carga_pala','horaini_turnocarga','elevacion_poligono_metros']\n",
    "\n",
    "# Crear un DataFrame con las variables específicas\n",
    "datos_subset_escalar = datos_subset[variables_especificas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7ddc59-d4bd-4c5f-a522-3128fd81be71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_min = datos_subset_escalar.min()\n",
    "X_max = datos_subset_escalar.max()\n",
    "X_min, X_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156a4892-e670-452d-8e0b-e2fbdb095d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4.2 Generamos la funcion para generar el escalamiento que dejara valores entre 0 y 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f43fa97-5af4-49ea-bdf4-914329386bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_max.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d3856d-2af1-44ab-a522-2200e6979e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_minimo = np.array([ 0.00000000e+00,  1.20000000e+01,  2.50000000e+01,  0.00000000e+00,\n",
    "        0.00000000e+00,  0.00000000e+00,  1.52800000e+02,  0.00000000e+00,\n",
    "       -2.42063328e+05, -1.00000000e+00,  7.00000000e+00,  4.64000000e+02])\n",
    "X_maximo = np.array([2.00000000e+07, 2.70000000e+01, 4.50000000e+01, 1.80000000e+03,\n",
    "       4.00000000e+02, 3.51744191e+03, 3.20600000e+02, 4.24100000e+00,\n",
    "       3.14238200e+03, 8.00000000e+00, 1.90000000e+01, 8.37000000e+02])\n",
    "def min_max_scale(data, X_min, X_max):\n",
    "    '''\n",
    "    Parámetros:\n",
    "    - data: Un array NumPy con los datos crudos.\n",
    "    - X_min: Un array NumPy con los valores mínimos para cada característica.\n",
    "    - X_max: Un array NumPy con los valores máximos para cada característica.\n",
    "    '''\n",
    "    data_scaled = (data - X_min) / (X_max - X_min)\n",
    "    return data_scaled\n",
    "datos_subset_escalados = min_max_scale(datos_subset_escalar,X_minimo, X_maximo)\n",
    "datos_subset_escalados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a99d08f-b596-4243-8225-3e9098319393",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3 Verificamos que no haya valores NULL luego de hacer el Escalamiento de Datos\n",
    "datos_subset_escalados.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453de7ba-690a-463c-a6c4-95e798d90280",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reemplazar las columnas escaladas en el DataFrame original\n",
    "datos_subset[variables_especificas] = datos_subset_escalados\n",
    "\n",
    "# Mostrar el resultado\n",
    "datos_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c59583-5447-4f37-bab4-1d99e2494e95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Finalmente tenemos nuestros datos escalados para variables especificos (incluidos las variables que no han sido escaldas)\n",
    "datos_total_escalados = datos_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed8cb19-11f3-4381-b08c-1159e51219ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Luego de Hacer el Feature Importances (Metodo de Envoltura) se identifico este conjunto de variables mas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a673062-fbc5-4a71-9084-894b89eb8013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Luego de Hacer el Feature Importances (Metodo de Envoltura) se identifico este conjunto de variables mas importantes\n",
    "datos_total_escalados_mlp =  datos_total_escalados[['tonelaje_inicial_poligono_creado',\n",
    "'tiempo_ready_llegada_esperando',\n",
    "'elevacion_poligono_metros',\n",
    "'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala',\n",
    "'densidad_inicial_poligono_creado_tn/m3',\n",
    "'horaini_turnocarga']]\n",
    "\n",
    "# Supongamos que datos es tu DataFrame y has seleccionado tus características (X) y variable objetivo (y)\n",
    "# Capacidad_peso = capacidad_volumen * densidad   O  capacidad_volumen = Capacidad_peso / densidad\n",
    "X = datos_total_escalados_mlp.values #Sale de los datos escalados\n",
    "\n",
    "y = datos['numero_pases_carguio'].values # Reemplaza 'variable_objetivo' con el nombre de tu variable objetivo(sale de los datos originales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c48be24-a087-4467-89cf-9176fac665e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.7 Red Neuronal MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a68d033-f73c-4ea8-a275-f4cfd8be12ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aseguramos que los resultados sean \"reproducibles\" en cada ejecucion de tensorflow(pesos iniciales aleatorios)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dddb6f3-2e9c-408f-bc97-c27472fb3501",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suponiendo que tienes tus datos X e y definidos antes de esta sección\n",
    "\n",
    "\n",
    "#Dividir los sets de entrenamiento, validacion, test\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set (Dejas el 0.75 para entrenamiento 1-0.75 = 0.25 quedan libres)\n",
    "x_train_mlp, x_test_mlp, y_train_mlp, y_test_mlp = train_test_split(X, y, test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set(de lo que sobro arriba 25%, sacas tu validacion y test)\n",
    "x_val_mlp, x_test_mlp, y_val_mlp, y_test_mlp = train_test_split(x_test_mlp, y_test_mlp, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "# ---  test_ratio/(test_ratio + validation_ratio) ---\n",
    "#Esto significa que del 25% de datos restantes, el 40% será para el conjunto de prueba y el 60% para el conjunto de validación.\n",
    "#Con esta proporción:\n",
    "#x_val_lstm y y_val_lstm contendrán 0.60 * 0.25 = 0.15 del total original.\n",
    "#x_test_lstm y y_test_lstm contendrán 0.40 * 0.25 = 0.10 del total original.\n",
    "\n",
    "\n",
    "# Crear el modelo de la red neuronal\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(10, input_dim=X.shape[1], activation='relu')) #input_dim : numero de variables inputs\n",
    "model_mlp.add(Dense(20, activation='relu'))\n",
    "model_mlp.add(Dense(10, activation='relu'))\n",
    "model_mlp.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model_mlp.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_mlp.summary()\n",
    "\n",
    "# Entrenar el modelo y registrar las pérdidas\n",
    "# Entrenar el modelo y registrar las pérdidas\n",
    "history = model_mlp.fit(x_train_mlp, y_train_mlp, epochs=50, batch_size=5, verbose=2, validation_data=(x_val_mlp, y_val_mlp))\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred_mlp = model_mlp.predict(x_test_mlp)\n",
    "y_pred_mlp = np.round(y_pred_mlp).astype('int64')\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse = mean_squared_error(y_test_mlp, y_pred_mlp)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab465e54-6649-4cc7-ae66-05ec7e995c57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vizualizar las Perdidas(Para detectar si hay Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350cad38-3296-437d-89c8-f84aeacf8137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Graficar las pérdidas de entrenamiento y validación\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.title('Curva de aprendizaje')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "395c5b37-f3fd-4485-9a8b-f2aa8a338c7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluar el rendimiento del Modelo en el Conj. de Entrenamiento y Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec1766e-6f6e-487c-ba1f-52b93ae9dd20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hacer predicciones en el conjunto de ENTRENAMIENTO\n",
    "y_train_pred_mlp = model_mlp.predict(x_train_mlp)\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de entrenamiento\n",
    "mse_train = mean_squared_error(y_train_mlp, y_train_pred_mlp)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Hacer predicciones en el conjunto de VALIDACION\n",
    "y_pred_val_mlp = model_mlp.predict(x_val_mlp)\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse_val = mean_squared_error(y_val_mlp, y_pred_val_mlp)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Hacer predicciones en el conjunto de PRUEBA\n",
    "y_pred_mlp = model_mlp.predict(x_test_mlp)\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse_test = mean_squared_error(y_test_mlp, y_pred_mlp)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de entrenamiento: {rmse_train}')\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de entrenamiento: {rmse_val}')\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f1f46df-29f7-4a87-98ac-4ab7729e3b15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Feature Importances in REDES NEURONALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "530ded9f-f015-4772-9303-da732ae1fab7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metodo 1: Metodo Basado en Gradientes\n",
    "- Gradiente: En términos matemáticos, el gradiente es una medida de cuánto cambia la salida de una función (en este caso, la red neuronal) en respuesta a cambios en sus entradas. Es un vector que apunta en la dirección del mayor aumento de la función.\n",
    "\n",
    "- Importancia de las características: La importancia de una característica se puede estimar observando la magnitud del gradiente de la salida con respecto a esa característica. Una mayor magnitud del gradiente sugiere que pequeños cambios en esa característica tienen un gran impacto en la salida, lo que indica una mayor importancia.\n",
    "\n",
    "- Calcular los gradientes: Una vez que la red neuronal está entrenada, se calculan los gradientes de la salida de la red con respecto a cada característica de entrada. Esto se hace usando el cálculo automático de derivadas, que es una capacidad estándar en bibliotecas de aprendizaje profundo como TensorFlow y PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52fc832b-68ee-448f-a744-ad7f0220d990",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('your_model.h5') # Load your trained model\n",
    "\n",
    "# X = np.load('your_input_data.npy') # Load your input data\n",
    "\n",
    "# Convertir X(datos de caracteristicas MLP Red Neuronal Multicapa) a tensor de TensorFlow\n",
    "input_tensor = tf.convert_to_tensor(X)\n",
    "\n",
    "# Calcular gradientes de la salida con respecto a las entradas\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_tensor)\n",
    "    output = model_mlp(input_tensor)\n",
    "\n",
    "gradients = tape.gradient(output, input_tensor)\n",
    "\n",
    "# Calcular la importancia de las características como la media absoluta de los gradientes\n",
    "feature_importance = np.mean(np.abs(gradients.numpy()), axis=0)\n",
    "\n",
    "# Obtener los nombres de las características del DataFrame original\n",
    "feature_names = datos_total_escalados_mlp.columns\n",
    "\n",
    "# Crear un DataFrame para asociar nombres de características con sus importancias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6babf58a-4162-4f03-a104-3652aa5b54cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metodo 2:  basado en pesos (weight-based methods) \n",
    "- se utiliza para estimar la importancia de las características en una red neuronal considerando los pesos o conexiones entre las neuronas\n",
    "- Extraer Pesos: Obtener los pesos de las capas del modelo, especialmente de la primera capa si se está evaluando la importancia de las características de entrada.\n",
    "-Calcular Importancia: Calcular la importancia de cada característica sumando los valores absolutos de sus pesos. Esto se basa en la idea de que los pesos con valores absolutos mayores tienen un impacto mayor en la salida del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee897e4-3b51-47e2-a434-3f9bc951e48b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los pesos de las capas del modelo\n",
    "weights = model_mlp.get_weights()\n",
    "\n",
    "# Calcular la importancia de las características como la suma de los valores absolutos de los pesos de la primera capa\n",
    "feature_importance = np.sum(np.abs(weights[0]), axis=1)\n",
    "\n",
    "# Obtener los nombres de las características del DataFrame original\n",
    "feature_names = datos_total_escalados_mlp.columns\n",
    "\n",
    "# Crear un DataFrame para asociar nombres de características con sus importancias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Importancia de las características:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "744e631f-b982-4714-b0e9-11716b9c4e00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Convertir el Modelo a Tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "026e88c9-93d1-49d6-96cf-385408b732e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Convertir desde la misma sesion y Configurar el convertidor TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22502d91-f110-436b-974a-37adb4fa16d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir desde la misma sesion y Configurar el convertidor TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model=model_mlp)\n",
    "\n",
    "# Especificar las operaciones admitidas por el modelo TFLite\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# Deshabilitar la conversión de operaciones de lista de tensores\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convertir el modelo a TensorFlow Lite\n",
    "model_tflite = converter.convert()\n",
    "\n",
    "# Guardar el modelo TensorFlow Lite en disco\n",
    "with open(\"model_mlp_v1.tflite\", \"wb\") as f:\n",
    "    f.write(model_tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab0f4f48-fb0b-43c6-9e87-81f9f1207539",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Convertir desde Cargar el modelo desde la ubicacion donde se guardo con \"SavedModel de TensorFlow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b6f02f-eaa5-4a2b-966d-2ea85d1d03af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Guardar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684ee706-ae49-4a26-b619-c0f465c68e35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "\n",
    "# Guardando todos los componentes del modelo (arquitectura, pesos, configuración de entrenamiento, estado del optimizador, etc.)\n",
    "# SavedModel de TensorFlow\n",
    "model_mlp.save(\"model_mlp_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2dcd64-179b-4a2a-b156-a99747bbe51b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Cargar el modelo desde la ubicacion donde se guardo con \"SavedModel de TensorFlow\"\n",
    "model_cargado = tf.keras.models.load_model('model_mlp_v1.h5')\n",
    "\n",
    "# Convertir el modelo a formato .tflite\n",
    "converter2 = tf.lite.TFLiteConverter.from_keras_model(model_cargado)\n",
    "\n",
    "# Especificar el conjunto de operaciones admitidas\n",
    "converter2.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Operaciones TensorFlow Lite estándar (mayoría de los modelos, como convoluciones, activaciones, etc.)\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS      # Operaciones TensorFlow adicionales - Esto debes instalar en el Android Studio Gradle\n",
    "]\n",
    "\n",
    "# Convertir el modelo\n",
    "tflite_model2 = converter2.convert()\n",
    "\n",
    "# Guardar el modelo convertido en formato .tflite\n",
    "with open('model_mlp_v1_2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.1.model_developer_2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
