{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063e75e3-48a1-4e46-a7f1-9c361ee98c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "\n",
    "#gf\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.tensorflow\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#import streamlit as st\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "#Librerias para redes neuronales(secuencial)\n",
    "from tensorflow.keras.models import Sequential\n",
    "#Capa completamente conectada\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "#Optimizador\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configurar pandas para mostrar todas las filas y columnas\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5628b485-a159-47c0-843a-258aeb80c045",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. Leer datos para crear los Modelos IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ee70a6-a9bd-4886-be5c-f42338b9cef8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/presentation/proyectopases_presentation/datos_presentation_tabladelta_bi\")\n",
    "datos = df_delta.toPandas()\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a948f49-daf5-4361-b324-c3f3f0de0f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b66875a4-6ce7-4de3-b993-27080ca1644b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Veamos el periodo de datos que estaremos utilizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c9c46f-5cca-4745-a03c-7e6a1fec224b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos['tiempo_inicio_carga_carguio'].min(), datos['tiempo_inicio_carga_carguio'].max(), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b4fe31-5c5f-47a6-afab-e8fed14ad898",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Veamos la dimension de los datos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50384814-97f8-4613-89a8-719c70a7c1b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4700c6a9-ef6d-45e0-918b-515d52d89726",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2. Hacemos un Tratamiento de variable categorica con One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a08fa8-7be7-4edc-8352-244372aafd4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.1 Metodo 1 - Usando la libreria de One Hot Encoder de Sckilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a96635-9b6d-437b-aa55-8f485367f8d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "# Aplicar el One Hot Encoding\n",
    "encoder = OneHotEncoder(sparse_output=False)   # Saprce para manejar el numero de ceros, en la generacion del one hot encoding\n",
    "one_hot = encoder.fit_transform(datos[['nombre_equipo_carguio']])\n",
    "\n",
    "# Guardar el codificador One Hot Encoding entrenado\n",
    "with open('one_hot_encoder_pases.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "one_hot_df = pd.DataFrame(one_hot, columns=encoder.get_feature_names_out(['nombre_equipo_carguio']))  #equipo_carguio , nombre acompanate en la columnas de los dummies\n",
    "\n",
    "# Unir el DataFrame original con el codificado\n",
    "datos = pd.concat([datos, one_hot_df], axis=1)\n",
    "\n",
    "# Mostrar el resultado\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7769a938-7575-424f-b0e4-451f01b48164",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Supongamos que se agrega una nueva fila a la base de datos\n",
    "nueva_fila = pd.DataFrame({'nombre_equipo_carguio': ['CF03']})\n",
    "\n",
    "# # Cargar el codificador guardado\n",
    "with open('one_hot_encoder_pases.pkl', 'rb') as f:\n",
    "     encoder = pickle.load(f)\n",
    "\n",
    "# # Transformar la nueva fila\n",
    "one_hot_nueva_fila = encoder.transform(nueva_fila)\n",
    "\n",
    "# # Convertir a DataFrame\n",
    "one_hot_nueva_fila_df = pd.DataFrame(one_hot_nueva_fila, columns=encoder.get_feature_names_out(['nombre_equipo_carguio']))\n",
    "\n",
    "one_hot_nueva_fila_df  # De aqui filtras las columnas y le vas pasando al Modelo de IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5403e14-22a4-4e91-96c7-89c778d42869",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2 Metodo 2 -  Usar un bucle para realizar el codigo de One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91eef228-f531-46fd-be79-8f28404e25a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "VARIABLE nombre_equipo_carguio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2da9e81-4030-4db6-b83b-529659abed48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "categorias = datos['nombre_equipo_carguio'].unique()\n",
    "\n",
    "# Crear un DataFrame vacío con columnas para cada categoría, Diccionario one_hot_dict : categoria: clave es una categoría única de #nombre_equipo_carguio, y el [0]*len(datos) : valor asociado a cada clave es una lista de ceros de la misma longitud que el DataFrame \n",
    "one_hot_dict = {categoria: [0]*len(datos) for categoria in categorias}\n",
    "\n",
    "# Llenar el diccionario con 1s en las posiciones correspondientes\n",
    "# idx es el índice de la fila, val es el valor de nombre_equipo_carguio en esa fila\n",
    "# Si idx=0 y val='CF02', entonces one_hot_dict['CF02'][0] = 1\n",
    "for idx, val in datos['nombre_equipo_carguio'].items():\n",
    "    one_hot_dict[val][idx] = 1\n",
    "\n",
    "# Convertir el diccionario a un DataFrame\n",
    "one_hot_df = pd.DataFrame(one_hot_dict)\n",
    "\n",
    "#Unir el DataFrame original con el codificado\n",
    "datos = pd.concat([datos, one_hot_df], axis=1)\n",
    "\n",
    "# # Mostrar el resultado\n",
    "datos[['nombre_equipo_carguio','CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783eb521-ce29-4a79-aa1e-57ad1c307c43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a09a636b-8b3a-40cc-bc8f-551755c24288",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "VARIABLE Procedencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de42a89-be54-4597-a9d5-1f254ce9b321",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener las categorías únicas\n",
    "categorias_pro = datos['Procedencia'].unique()\n",
    "\n",
    "# Crear un DataFrame vacío con columnas para cada categoría, Diccionario one_hot_dict : categoria: clave es una categoría única de #nombre_equipo_carguio, y el [0]*len(datos) : valor asociado a cada clave es una lista de ceros de la misma longitud que el DataFrame \n",
    "one_hot_dict_pro = {categoria: [0]*len(datos) for categoria in categorias_pro}\n",
    "\n",
    "# Llenar el diccionario con 1s en las posiciones correspondientes\n",
    "# idx es el índice de la fila, val es el valor de nombre_equipo_carguio en esa fila\n",
    "# Si idx=0 y val='CF02', entonces one_hot_dict['CF02'][0] = 1\n",
    "for idx, val in datos['Procedencia'].items():\n",
    "    one_hot_dict_pro[val][idx] = 1\n",
    "\n",
    "# Convertir el diccionario a un DataFrame\n",
    "one_hot_dict_pro_df = pd.DataFrame(one_hot_dict_pro)\n",
    "\n",
    "#Unir el DataFrame original con el codificado\n",
    "datos = pd.concat([datos, one_hot_dict_pro_df], axis=1)\n",
    "\n",
    "# # Mostrar el resultado\n",
    "datos[['Procedencia','TAJ', 'SUL', 'OXI', 'NoDefinido']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb446a30-cd1c-43e4-8c55-06c0f9c53c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. Hacemos un tratamiento de variable Boleana para convertir a Numerica por el Metodo del Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28f56d06-daca-4b4e-a1eb-e76f474430e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Variable al_termino_cargar_en_espera_cuadrado_cuadrandose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9daafa3d-eea1-4afe-aeac-cb90d1a909ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Crear un diccionario para mapear valores booleanos a numéricos\n",
    "bool_to_num = {True: 1, False: 0}\n",
    "\n",
    "# # Aplicar el diccionario a la columna booleana para convertirla a numérica\n",
    "datos['al_termino_cargar_en_espera_cuadrado_cuadrandose'] = datos['al_termino_cargar_en_espera_cuadrado_cuadrandose'].map(bool_to_num)\n",
    "\n",
    "# Mostrar el resultado\n",
    "datos['al_termino_cargar_en_espera_cuadrado_cuadrandose'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50107efb-ed6c-489c-9238-4d388810f23b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Seleccion de Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0879aab3-ebb9-4a25-bc90-fdf262926870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3.1 Seleccion variables con  StatModels, para regresion lineal (variables significativas estadisticamente basado en la Hipotesis Nula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29dc48f-0908-4fde-9ec4-6be1e26a4320",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Veamos el tipo de datos que estaremos utilizando\n",
    "datos[['tonelaje_inicial_poligono_creado','radiohexagonocuchara_equipocarguio',\n",
    "'capacidad_en_volumen_equipo_carguio_m3','capacidad_en_volumen_equipo_acarreo_m3',\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "'capacidad_en_peso_equipo_acarreo',\n",
    "'tiempo_estimado_duracion_estado_pala',\n",
    "'radio_locacion_metros',\n",
    "'tiempo_ready_llegada_esperando',\n",
    "'tonelaje_camion_antes_cargaestabilizada',\n",
    "'tonelaje_segun_computadora',\n",
    "'densidad_inicial_poligono_creado_tn/m3','tiempo_carga','al_termino_cargar_en_espera_cuadrado_cuadrandose',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala','horaini_turnocarga','elevacion_poligono_metros',\n",
    "'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "'TAJ', 'SUL', 'OXI', 'NoDefinido']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34abcecd-572e-4be3-9805-fc31cac74d03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos['numero_pases_carguio'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5998f2ea-c997-475f-a61c-c8ad784aed9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Supongamos que datos es tu DataFrame y has seleccionado tus características (X) y variable objetivo (y)\n",
    "X = datos[['tonelaje_inicial_poligono_creado','radiohexagonocuchara_equipocarguio',\n",
    "'capacidad_en_volumen_equipo_carguio_m3','capacidad_en_volumen_equipo_acarreo_m3',\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "'capacidad_en_peso_equipo_acarreo',\n",
    "'tiempo_estimado_duracion_estado_pala',\n",
    "'radio_locacion_metros',\n",
    "'tiempo_ready_llegada_esperando',\n",
    "'tonelaje_camion_antes_cargaestabilizada',\n",
    "'tonelaje_segun_computadora',\n",
    "'densidad_inicial_poligono_creado_tn/m3','tiempo_carga','al_termino_cargar_en_espera_cuadrado_cuadrandose',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala','horaini_turnocarga','elevacion_poligono_metros',\n",
    "'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "'TAJ', 'SUL', 'OXI', 'NoDefinido']]  # Reemplaza 'feature1', 'feature2', 'feature3' con tus nombres de características\n",
    "y = datos['numero_pases_carguio'] # Reemplaza 'variable_objetivo' con el nombre de tu variable objetivo\n",
    "\n",
    "# Añadir constante a las características (intercepto)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Crear el modelo de regresión lineal con statsmodels\n",
    "modelo = sm.OLS(y, X).fit()\n",
    "\n",
    "# Imprimir un resumen estadistico del modelo\n",
    "print(modelo.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a045dd-c6e0-49d1-b5f0-78d284d45b59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Variables descartadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f4e05d-9f90-4f35-8d7e-afa3ae50f632",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# capacidad_en_volumen_equipo_carguio_m3             \n",
    "# capacidad_en_volumen_equipo_acarreo_m3            \n",
    "# capacidad_en_peso_equipo_carguio                    \n",
    "# capacidad_en_peso_equipo_acarreo   \n",
    "# horaini_turnocarga      \n",
    "# FR01            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f2fad09-d2e3-49d2-8980-6867c822f8a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3.2 Ver Correlaciones entre mis variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c256c4b-965a-42f7-9404-d7d66e17ac5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que tienes un DataFrame llamado 'datos'\n",
    "\n",
    "# Lista de variables que deseas analizar\n",
    "variables_a_analizar = ['tonelaje_inicial_poligono_creado','radiohexagonocuchara_equipocarguio',\n",
    "'capacidad_en_volumen_equipo_carguio_m3','capacidad_en_volumen_equipo_acarreo_m3',\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "'capacidad_en_peso_equipo_acarreo',\n",
    "'tiempo_estimado_duracion_estado_pala',\n",
    "'radio_locacion_metros',\n",
    "'tiempo_ready_llegada_esperando',\n",
    "'tonelaje_camion_antes_cargaestabilizada',\n",
    "'tonelaje_segun_computadora',\n",
    "'densidad_inicial_poligono_creado_tn/m3','tiempo_carga','al_termino_cargar_en_espera_cuadrado_cuadrandose',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala','horaini_turnocarga','elevacion_poligono_metros',\n",
    "'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "'TAJ', 'SUL', 'OXI', 'NoDefinido','numero_pases_carguio']\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las variables de interés\n",
    "datos_subset = datos[variables_a_analizar]\n",
    "\n",
    "# Crear una matriz de correlación para el subconjunto de variables\n",
    "matriz_correlacion = datos_subset.corr()\n",
    "\n",
    "# Configurar el estilo de Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Aumentar el tamaño del gráfico para mostrar más detalles\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Crear un mapa de calor (heatmap) de la matriz de correlación con desplazamiento habilitado\n",
    "ax = sns.heatmap(matriz_correlacion, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=.5)\n",
    "\n",
    "# Agregar etiquetas a los ejes X e Y\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right', fontsize=8)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, horizontalalignment='right', fontsize=8)\n",
    "\n",
    "# Ajustar el tamaño de las fuentes de los nombres de las variables (eje x)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "\n",
    "# Ajustar el tamaño de las fuentes de los nombres de las variables (eje y)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "plt.title(\"Matriz de Correlación Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "829c1670-6522-45d5-b605-a177c5c7069e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variables seleccionadas con 20% de correlacion o superior\n",
    "# 'radiohexagonocuchara_equipocarguio',\n",
    "# 'capacidad_en_volumen_equipo_carguio_m3',\n",
    "# 'capacidad_en_peso_equipo_carguio',\n",
    "# 'tiempo_estimado_duracion_estado_pala',\n",
    "# 'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "# 'TAJ', 'SUL', 'OXI', 'NoDefinido'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8842f6c9-8b99-4a67-bd86-2eceb2a9f6b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3.3 Ver la varianza de mis variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9f62c3-67d1-467d-b93a-5e1b9178efdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_varianza=datos[['tonelaje_inicial_poligono_creado','radiohexagonocuchara_equipocarguio',\n",
    "'capacidad_en_volumen_equipo_carguio_m3','capacidad_en_volumen_equipo_acarreo_m3',\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "'capacidad_en_peso_equipo_acarreo',\n",
    "'tiempo_estimado_duracion_estado_pala',\n",
    "'radio_locacion_metros',\n",
    "'tiempo_ready_llegada_esperando',\n",
    "'tonelaje_camion_antes_cargaestabilizada', 'tonelaje_segun_computadora',\n",
    "'densidad_inicial_poligono_creado_tn/m3','tiempo_carga','al_termino_cargar_en_espera_cuadrado_cuadrandose',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala','horaini_turnocarga','elevacion_poligono_metros',\n",
    "'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "'TAJ', 'SUL', 'OXI', 'NoDefinido']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55aa08eb-bf33-4044-8478-60bf4a4637b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "desviacion_estandar_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbfb33d-85fe-4972-a732-c65a810738f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Varibales a descartar porque no aportan informacion al Modelo\n",
    "#capacidad_en_peso_equipo_acarreo                         0.000000\n",
    "# capacidad_en_volumen_equipo_acarreo_m3                   0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7197b4a-28ed-41a8-b618-9e2815135f78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la varianza de cada columna\n",
    "desviacion_estandar = np.sqrt(datos_varianza.var())  #desviacion estandar \n",
    "\n",
    "# Redondear la varianza a 2 decimales\n",
    "#variance_rounded = variance.round(2)\n",
    "\n",
    "# Ordenar las varianzas de mayor a menor\n",
    "desviacion_estandar_sorted = desviacion_estandar.sort_values(ascending=False)\n",
    "\n",
    "# Graficar los resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "desviacion_estandar_sorted.plot(kind='bar')\n",
    "plt.title('Varianza de Variables del DataFrame')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Varianza')\n",
    "# plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc487907-a46d-41ef-b211-e764f6ef31a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Variables Seleccionadas antes de entrenar los modelos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca8496b-7849-4815-bf9d-49203bad8ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_subset=datos[['tonelaje_inicial_poligono_creado','radiohexagonocuchara_equipocarguio',\n",
    "'capacidad_en_volumen_equipo_carguio_m3',\n",
    "#'capacidad_en_volumen_equipo_acarreo_m3', #varianza 0\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "#'capacidad_en_peso_equipo_acarreo',  # varianza 0\n",
    "'tiempo_estimado_duracion_estado_pala',\n",
    "'radio_locacion_metros',\n",
    "'tiempo_ready_llegada_esperando',\n",
    "'tonelaje_camion_antes_cargaestabilizada',\n",
    "'tonelaje_segun_computadora',\n",
    "'densidad_inicial_poligono_creado_tn/m3','tiempo_carga','al_termino_cargar_en_espera_cuadrado_cuadrandose',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala','horaini_turnocarga','elevacion_poligono_metros',\n",
    "'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "'TAJ', 'SUL', 'OXI', 'NoDefinido']]\n",
    "datos_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4439f438-631a-4e0c-8cc8-a184ece32c10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###4. Funcion para Escalar datos, y guardar los valores para escalar en produccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "812c364f-7d18-4e20-b48e-ceb7ea76b2e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4.1 Hallamos los Minimos y Maximos de cada Variable a escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e728b3fa-632c-4291-8d0c-86d143b51396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecciona las variables específicas\n",
    "variables_especificas = ['tonelaje_inicial_poligono_creado','radiohexagonocuchara_equipocarguio',\n",
    "                         'capacidad_en_volumen_equipo_carguio_m3',\n",
    "                         'capacidad_en_peso_equipo_carguio',\n",
    "                         'tiempo_estimado_duracion_estado_pala','radio_locacion_metros',\n",
    "                         'tiempo_ready_llegada_esperando','tonelaje_camion_antes_cargaestabilizada',\n",
    "                         'tonelaje_segun_computadora',\n",
    "                         'densidad_inicial_poligono_creado_tn/m3',\n",
    "                         'tiempo_carga',\n",
    "                         'cantidad_equipos_espera_al_termino_carga_pala','horaini_turnocarga','elevacion_poligono_metros']\n",
    "\n",
    "# Crear un DataFrame con las variables específicas\n",
    "datos_subset_escalar = datos_subset[variables_especificas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba3d43e-6df6-49e9-b050-99bdde1e1dc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_min = datos_subset_escalar.min()\n",
    "X_max = datos_subset_escalar.max()\n",
    "X_min, X_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3760cf83-fdd2-4eda-ad98-693bfbaff77c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4.2 Generamos la funcion para generar el escalamiento que dejara valores entre 0 y 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd781f10-488e-411e-b4be-099bd95dc1ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_max.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bacf968-45b6-47a1-b3ca-899b043c3c47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# X_minimo = np.array([-242063.328,\n",
    "#                      12.000, \n",
    "#                      0.000,\n",
    "#                      464.000,\n",
    "#                     0.000,\n",
    "#                      -1.000 ])\n",
    "# X_maximo = np.array([3.142382e+03,\n",
    "#                      2.700000e+01, \n",
    "#    4.241000e+00, \n",
    "#    8.370000e+02,   \n",
    "#    2.000000e+07,\n",
    "#    8.000000e+00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f75e6b-6fd4-4004-b95f-3faa8c12696e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_minimo = np.array([  0.00000000e+00,  0.00000000e+00,  1.20000000e+01,  2.50000000e+01,\n",
    "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
    "        1.52800000e+02,  0.00000000e+00, -2.42063328e+05, -1.00000000e+00,\n",
    "        7.00000000e+00,  4.64000000e+02])\n",
    "X_maximo = np.array([2.00000000e+07, 6.80000000e+02, 2.70000000e+01, 4.50000000e+01,\n",
    "       1.80000000e+03, 4.00000000e+02, 3.51744191e+03, 3.09900000e+03,\n",
    "       3.20600000e+02, 4.24100000e+00, 3.14238200e+03, 8.00000000e+00,\n",
    "       1.90000000e+01, 8.37000000e+02])\n",
    "def min_max_scale(data, X_min, X_max):\n",
    "    '''\n",
    "    Parámetros:\n",
    "    - data: Un array NumPy con los datos crudos.\n",
    "    - X_min: Un array NumPy con los valores mínimos para cada característica.\n",
    "    - X_max: Un array NumPy con los valores máximos para cada característica.\n",
    "    '''\n",
    "    data_scaled = (data - X_min) / (X_max - X_min)\n",
    "    return data_scaled\n",
    "datos_subset_escalados = min_max_scale(datos_subset_escalar,X_minimo, X_maximo)\n",
    "datos_subset_escalados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e45e384-0c8d-4588-8f5e-e83cabf12310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3 Verificamos que no haya valores NULL luego de hacer el Escalamiento de Datos\n",
    "datos_subset_escalados.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e665793-8fb5-4437-afe5-2617af8dd68d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reemplazar las columnas escaladas en el DataFrame original\n",
    "datos_subset[variables_especificas] = datos_subset_escalados\n",
    "\n",
    "# Mostrar el resultado\n",
    "datos_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483524f5-dfa5-4b40-beec-df6bebc1e7f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Finalmente tenemos nuestros datos escalados para variables especificos (incluidos las variables que no han sido escaldas)\n",
    "datos_total_escalados = datos_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdcaaf4b-778a-4c34-b120-76bee6190399",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4.3 Probando como escalariamos en produccion, real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0406e961-2cb5-476f-ac70-ee5643035ff3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Supongamos que esta es la fila de entrada con los valores crudos\n",
    "X_crudo = np.array([15, 7, 2.0, 45, 15])\n",
    "X_crudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601dd54f-779f-43e2-ac1e-4719610a9862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "min = np.array([12.,  0.,  0.,  0.,  0.])\n",
    "max = np.array([2.700000e+01, 4.241000e+00, 5.012661e+03, 2.000000e+07,\n",
    "       4.000000e+02])\n",
    "X_crudo = np.array([15, 7, 2.0, 45, 15])\n",
    "min_max_scale(X_crudo,min,max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd4832c8-f6f3-415b-9dc3-9a2e4ca844a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Implementacion de Modelos de Machine learning y Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33148aac-089c-481a-ac54-55d1772572f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.1 Seleccionado tus características (X) y variable objetivo (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99601acc-3734-435b-a466-1fd1817c2250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_total_escalados.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba712068-3b53-450f-ba4a-3e265a14ba10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_total_escalados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffc245da-b033-4197-8949-bf8997940197",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Luego de Hacer el Feature Importances (Metodo de Envoltura) se identifico este conjunto de variables mas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57000a08-2886-4297-8d1c-8d1747fa14f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Luego de Hacer el Feature Importances (Metodo de Envoltura) se identifico este conjunto de variables mas importantes\n",
    "datos_total_escalados_variablesML =  datos_total_escalados[['tiempo_carga',  \n",
    "'capacidad_en_volumen_equipo_carguio_m3', 'densidad_inicial_poligono_creado_tn/m3',\n",
    "'TAJ', 'SUL', 'OXI', 'NoDefinido','cantidad_equipos_espera_al_termino_carga_pala','tonelaje_inicial_poligono_creado',\n",
    "'elevacion_poligono_metros','capacidad_en_peso_equipo_carguio']]\n",
    "\n",
    "# Supongamos que datos es tu DataFrame y has seleccionado tus características (X) y variable objetivo (y)\n",
    "# Capacidad_peso = capacidad_volumen * densidad   O  capacidad_volumen = Capacidad_peso / densidad\n",
    "X = datos_total_escalados_variablesML.values #Sale de los datos escalados\n",
    "\n",
    "y = datos['numero_pases_carguio'].values # Reemplaza 'variable_objetivo' con el nombre de tu variable objetivo(sale de los datos originales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4635790-0937-482d-a204-7ef6908d88fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "057b1560-02c9-40eb-8132-0ed3c8883624",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.2 Regresion Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5dba63-5982-49ee-9744-2b68bae9418b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train_rl, X_test_rl, y_train_rl, y_test_rl = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo de regresión lineal múltiple\n",
    "modelo_rl = LinearRegression()\n",
    "\n",
    "# Entrenar el modelo\n",
    "modelo_rl.fit(X_train_rl, y_train_rl)\n",
    "\n",
    "# Realizar predicciones en el conjunto de entrenamiento y prueba\n",
    "y_pred_train_rl = modelo_rl.predict(X_train_rl)\n",
    "y_pred_test_rl = modelo_rl.predict(X_test_rl)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_train_rl = np.round(y_pred_train_rl).astype('int64')\n",
    "y_pred_test_rl = np.round(y_pred_test_rl).astype('int64')\n",
    "\n",
    "# Medir el rendimiento del modelo en el conjunto de entrenamiento\n",
    "# mae_train = metrics.mean_absolute_error(y_train_rl, y_pred_train_rl)\n",
    "mse_train = metrics.mean_squared_error(y_train_rl, y_pred_train_rl)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "# r2_train = metrics.r2_score(y_train_rl, y_pred_train_rl)\n",
    "\n",
    "# Medir el rendimiento del modelo en el conjunto de prueba\n",
    "# mae_test = metrics.mean_absolute_error(y_test_rl, y_pred_test_rl)\n",
    "mse_test = metrics.mean_squared_error(y_test_rl, y_pred_test_rl)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "# r2_test = metrics.r2_score(y_test_rl, y_pred_test_rl)\n",
    "\n",
    "# Imprimir métricas de rendimiento en ambos conjuntos\n",
    "print(\"Métricas de rendimiento en el conjunto de entrenamiento:\")\n",
    "# print(f'Error Absoluto Medio (MAE): {mae_train}')\n",
    "# print(f'Error Cuadrático Medio (MSE): {mse_train}')\n",
    "print(f'Raíz del Error Cuadrático Medio (RMSE): {rmse_train}')\n",
    "# print(f'Coeficiente de Determinación (R^2): {r2_train}')\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Métricas de rendimiento en el conjunto de prueba:\")\n",
    "# print(f'Error Absoluto Medio (MAE): {mae_test}')\n",
    "# print(f'Error Cuadrático Medio (MSE): {mse_test}')\n",
    "print(f'Raíz del Error Cuadrático Medio (RMSE): {rmse_test}')\n",
    "# print(f'Coeficiente de Determinación (R^2): {r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52e61094-51d8-494c-ae44-765c2d83907e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Un RMSE de 1.22 significa que, en promedio, las predicciones tienen un error de alrededor de 1.52 unidades  de nuemro de pases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f813daa0-0a81-403a-a0a9-60718adf1c0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Crear un gráfico de valores reales vs predichos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0717acbf-0b3d-41eb-99fb-236092158c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizar las predicciones\n",
    "plt.scatter(y_test_rl, y_pred_test_rl)\n",
    "plt.xlabel('Real [Pases]')\n",
    "plt.ylabel('Prediccion [Pases]')\n",
    "lims = [0, 20]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b209d54-eef9-486b-9b76-57cf100a97c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.3 Arbol de decision Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e5335f-8532-4dec-acbe-2e20518bed43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Luego de Hacer el Feature Importances (Metodo de Envoltura) se identifico este conjunto de variables mas importantes\n",
    "datos_total_escalados_dt =  datos_total_escalados[['tiempo_carga',\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "'densidad_inicial_poligono_creado_tn/m3',\n",
    "'tonelaje_inicial_poligono_creado',\n",
    "'elevacion_poligono_metros',\n",
    "'capacidad_en_volumen_equipo_carguio_m3']]\n",
    "\n",
    "# Supongamos que datos es tu DataFrame y has seleccionado tus características (X) y variable objetivo (y)\n",
    "# Capacidad_peso = capacidad_volumen * densidad   O  capacidad_volumen = Capacidad_peso / densidad\n",
    "X = datos_total_escalados_dt.values #Sale de los datos escalados\n",
    "\n",
    "y = datos['numero_pases_carguio'].values # Reemplaza 'variable_objetivo' con el nombre de tu variable objetivo(sale de los datos originales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c1b3734-0127-46b8-ab41-4c6868393b72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que X es tu conjunto de características y y es tu variable objetivo (Número de Pases)\n",
    "# Asegúrate de tener definidas estas variables antes de ejecutar el código\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo de árbol de decisión para regresión\n",
    "model_arbol = DecisionTreeRegressor(max_depth=5,random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_arbol.fit(X_train_dt, y_train_dt)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "# Realizar predicciones en el conjunto de train\n",
    "y_pred_dt_train = model_arbol.predict(X_train_dt)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_dt_train = np.round(y_pred_dt_train).astype('int64')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred_dt_test = model_arbol.predict(X_test_dt)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_dt_test = np.round(y_pred_dt_test).astype('int64')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Visualizar el árbol de decisión (opcional)\n",
    "# Nota: Necesitarás tener instalada la biblioteca graphviz y pydotplus para esto\n",
    "# from sklearn.tree import export_graphviz\n",
    "# import pydotplus\n",
    "# dot_data = export_graphviz(modelo_arbol, out_file=None, feature_names=X.columns, filled=True, rounded=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "# graph.write_png('arbol_decision.png')\n",
    "\n",
    "# Medir el rendimiento del modelo\n",
    "# error_absoluto_medio_dt = metrics.mean_absolute_error(y_test_dt, y_pred_dt)\n",
    "# error_cuadratico_medio_dt = metrics.mean_squared_error(y_test_dt, y_pred_dt)\n",
    "raiz_error_cuadratico_medio_dt_train = metrics.mean_squared_error(y_train_dt, y_pred_dt_train, squared=False) #squared=False indica a la #función que calcule la raíz cuadrada \n",
    "raiz_error_cuadratico_medio_dt_test = metrics.mean_squared_error(y_test_dt, y_pred_dt_test, squared=False)\n",
    "\n",
    "# Imprimir métricas de rendimiento\n",
    "# print(f'Error Absoluto Medio: {error_absoluto_medio_dt}')\n",
    "# print(f'Error Cuadrático Medio: {error_cuadratico_medio_dt}')\n",
    "print(f'Raíz del Error Cuadrático Medio Train: {raiz_error_cuadratico_medio_dt_train}')\n",
    "print(f'Raíz del Error Cuadrático Medio Test: {raiz_error_cuadratico_medio_dt_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2361d291-0efa-40c9-898e-0a81e7ec10d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Extraer mediante el metodo de envoltura las \"Variables mas importantes para el modelo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64ad23f-6efc-4c8c-a2c1-ce0b4ad41c4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener las importancias de las características\n",
    "feature_importances_dt = model_arbol.feature_importances_\n",
    "\n",
    "# Crear un DataFrame para mostrar las importancias de las características junto con sus nombres\n",
    "feature_importance_df_dt = pd.DataFrame({\n",
    "    'Feature': datos_total_escalados_dt.columns,\n",
    "    'Importance': feature_importances_dt\n",
    "})\n",
    "\n",
    "# Ordenar el DataFrame por importancia en orden descendente\n",
    "feature_importance_df_dt = feature_importance_df_dt.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 10 características más importantes\n",
    "print(feature_importance_df_dt.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2038ad8f-02bd-4443-9344-8c50b6897c27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener las importancias de las características\n",
    "feature_importances_dt = model_arbol.feature_importances_\n",
    "\n",
    "# Crear un DataFrame para mostrar las importancias de las características junto con sus nombres\n",
    "feature_importance_df_dt = pd.DataFrame({\n",
    "    'Feature': datos_total_escalados.columns,\n",
    "    'Importance': feature_importances_dt\n",
    "})\n",
    "\n",
    "# Ordenar el DataFrame por importancia en orden descendente\n",
    "feature_importance_df_dt = feature_importance_df_dt.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 10 características más importantes\n",
    "print(feature_importance_df_dt.head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "283a51b1-2178-430b-be3f-9c67fd8d52cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.4 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27504322-bc09-4833-ad88-ba3466c39310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Luego de Hacer el Feature Importances (Metodo de Envoltura) se identifico este conjunto de variables mas importantes\n",
    "datos_total_escalados_rf =  datos_total_escalados[['tiempo_carga','capacidad_en_volumen_equipo_carguio_m3',\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "'densidad_inicial_poligono_creado_tn/m3',\n",
    "'tonelaje_inicial_poligono_creado',\n",
    "'elevacion_poligono_metros',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala']]\n",
    "\n",
    "# Supongamos que datos es tu DataFrame y has seleccionado tus características (X) y variable objetivo (y)\n",
    "# Capacidad_peso = capacidad_volumen * densidad   O  capacidad_volumen = Capacidad_peso / densidad\n",
    "X = datos_total_escalados_rf.values #Sale de los datos escalados\n",
    "\n",
    "y = datos['numero_pases_carguio'].values # Reemplaza 'variable_objetivo' con el nombre de tu variable objetivo(sale de los datos originales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70ebb0b-ae0a-49ab-9a9e-3a0485f3eb1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que tienes un DataFrame llamado df con tus datos, y la columna objetivo es 'target'\n",
    "# Asegúrate de tener las características adecuadas para entrenar el modelo\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "model_rf = RandomForestRegressor(n_estimators=20, max_depth=5 ,random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_rf.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "# Realizar predicciones en el conjunto de train\n",
    "y_pred_rf_train = model_rf.predict(X_train_rf)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_rf_train = np.round(y_pred_rf_train).astype('int64')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred_rf_test = model_rf.predict(X_test_rf)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_rf_test = np.round(y_pred_rf_test).astype('int64')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Medir el rendimiento del modelo\n",
    "# error_absoluto_medio_dt = metrics.mean_absolute_error(y_test_dt, y_pred_dt)\n",
    "# error_cuadratico_medio_dt = metrics.mean_squared_error(y_test_dt, y_pred_dt)\n",
    "raiz_error_cuadratico_medio_rf_train = metrics.mean_squared_error(y_train_rf, y_pred_rf_train, squared=False) #squared=False indica a la #función que calcule la raíz cuadrada \n",
    "raiz_error_cuadratico_medio_rf_test = metrics.mean_squared_error(y_test_rf, y_pred_rf_test, squared=False)\n",
    "\n",
    "# Imprimir métricas de rendimiento\n",
    "# print(f'Error Absoluto Medio: {error_absoluto_medio_dt}')\n",
    "# print(f'Error Cuadrático Medio: {error_cuadratico_medio_dt}')\n",
    "print(f'Raíz del Error Cuadrático Medio Train: {raiz_error_cuadratico_medio_rf_train}')\n",
    "print(f'Raíz del Error Cuadrático Medio Test: {raiz_error_cuadratico_medio_rf_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78f6cb3b-6818-49ce-884e-acd415e5b8f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Extraer mediante el metodo de envoltura las \"Variables mas importantes para el modelo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bf1b58-4746-4d41-8667-81c11c26a3a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener las importancias de las características\n",
    "feature_importances_rf = model_rf.feature_importances_\n",
    "\n",
    "# Crear un DataFrame para mostrar las importancias de las características junto con sus nombres\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': datos_total_escalados_rf.columns,\n",
    "    'Importance': feature_importances_rf\n",
    "})\n",
    "\n",
    "# Ordenar el DataFrame por importancia en orden descendente\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 10 características más importantes\n",
    "print(feature_importance_df_rf.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0330f8f4-014e-4a20-84fc-056ea2aaf327",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener las importancias de las características\n",
    "feature_importances_rf = model_rf.feature_importances_\n",
    "\n",
    "# Crear un DataFrame para mostrar las importancias de las características junto con sus nombres\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': datos_total_escalados.columns,\n",
    "    'Importance': feature_importances_rf\n",
    "})\n",
    "\n",
    "# Ordenar el DataFrame por importancia en orden descendente\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 10 características más importantes\n",
    "print(feature_importance_df_rf.head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b5b0cb8-f2ae-41e0-b4de-f16cfdd02f75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.5 SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e135e2-c204-4d32-9c71-6a8db779bed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de SVM Regressor\n",
    "model_svm = SVR(kernel='linear')  # Puedes ajustar los hiperparámetros según sea necesario\n",
    "model_svm.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba con SVM Regressor\n",
    "y_pred_svm = model_svm.predict(X_test_svm)\n",
    "y_pred_svm = np.round(y_pred_svm).astype('int64')\n",
    "\n",
    "# Medir el rendimiento del modelo de SVM Regressor\n",
    "error_absoluto_medio_svm = metrics.mean_absolute_error(y_test_svm, y_pred_svm)\n",
    "error_cuadratico_medio_svm = metrics.mean_squared_error(y_test_svm, y_pred_svm)\n",
    "raiz_error_cuadratico_medio_svm = metrics.mean_squared_error(y_test_svm, y_pred_svm, squared=False)\n",
    "\n",
    "# Imprimir métricas de rendimiento de SVM Regressor\n",
    "print(\"\\nMétricas de rendimiento para SVM Regressor:\")\n",
    "print(f'Error Absoluto Medio: {error_absoluto_medio_svm}')\n",
    "print(f'Error Cuadrático Medio: {error_cuadratico_medio_svm}')\n",
    "print(f'Raíz del Error Cuadrático Medio: {raiz_error_cuadratico_medio_svm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5baa45a3-d665-4582-b97c-066e99d1fd71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.6 XGBoost: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a38764ca-61e0-4a57-9336-2db9f90c2296",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Construye árboles de manera secuencial. Cada árbol se entrena para corregir los errores de los árboles anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc55d1a2-eeee-47c2-b695-eb4c1a35cb71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Luego de Hacer el Feature Importances (Metodo de Envoltura) se identifico este conjunto de variables mas importantes\n",
    "datos_total_escalados_xgb =  datos_total_escalados[['tiempo_carga','capacidad_en_volumen_equipo_carguio_m3',\n",
    "'densidad_inicial_poligono_creado_tn/m3','elevacion_poligono_metros','tonelaje_inicial_poligono_creado',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala']]\n",
    "\n",
    "# Supongamos que datos es tu DataFrame y has seleccionado tus características (X) y variable objetivo (y)\n",
    "# Capacidad_peso = capacidad_volumen * densidad   O  capacidad_volumen = Capacidad_peso / densidad\n",
    "X = datos_total_escalados_xgb.values #Sale de los datos escalados\n",
    "\n",
    "y = datos['numero_pases_carguio'].values # Reemplaza 'variable_objetivo' con el nombre de tu variable objetivo(sale de los datos originales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ea5fb5-0f03-43f8-a790-9bcf062fb0a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Metodo 1,  Sin convertir a ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4053db6-5db1-46a9-ae91-79f8000f3560",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instalar las bibliotecas necesarias\n",
    "# pip install xgboost scikit-learn matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que tienes un DataFrame llamado df con tus datos, y la columna objetivo es 'target'\n",
    "# Asegúrate de tener las características adecuadas para entrenar el modelo\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Crear el modelo de XGBoost\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "    n_estimators=300,  # Número de árboles\n",
    "    max_depth=15,  # Profundidad máxima de los árboles\n",
    "    learning_rate=0.03, # tasa de aprendizaje, mas lento en ajustarse, pero necesitarias mas arboles\n",
    "    subsample = 0.7,\n",
    "    #reg_alpha = 0.5,\n",
    "    reg_lambda = 2.5, # regulazirzacion L2 Ridge\n",
    "    random_state=42 #Fijar la aleatoriedad durante el entrenamiento \n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_xgb.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "# Realizar predicciones en el conjunto de train\n",
    "y_pred_xgb_train = model_xgb.predict(X_train_xgb)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_xgb_train = np.round(y_pred_xgb_train).astype('int64')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred_xgb_test = model_xgb.predict(X_test_xgb)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_xgb_test = np.round(y_pred_xgb_test).astype('int64')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Medir el rendimiento del modelo\n",
    "# error_absoluto_medio_dt = metrics.mean_absolute_error(y_test_dt, y_pred_dt)\n",
    "# error_cuadratico_medio_dt = metrics.mean_squared_error(y_test_dt, y_pred_dt)\n",
    "raiz_error_cuadratico_medio_xgb_train = metrics.mean_squared_error(y_train_xgb, y_pred_xgb_train, squared=False) #squared=False indica a la #función que calcule la raíz cuadrada \n",
    "raiz_error_cuadratico_medio_xgb_test = metrics.mean_squared_error(y_test_xgb, y_pred_xgb_test, squared=False)\n",
    "\n",
    "# Imprimir métricas de rendimiento\n",
    "# print(f'Error Absoluto Medio: {error_absoluto_medio_dt}')\n",
    "# print(f'Error Cuadrático Medio: {error_cuadratico_medio_dt}')\n",
    "print(f'Raíz del Error Cuadrático Medio Train: {raiz_error_cuadratico_medio_xgb_train}')\n",
    "print(f'Raíz del Error Cuadrático Medio Test: {raiz_error_cuadratico_medio_xgb_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c56e3e7-4f78-49ea-bebc-567f092c558b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Metodo 2,  Con convertir a ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea203215-6204-466d-88a6-754d970fbcfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  #Libreria para manejo de operaciones matematicas y matrices\n",
    "#pip install onnxruntime -- Instalamos esta libreria\n",
    "import onnxruntime as rt   # operaciones con onnxruntime, para cargar el modelo.onnx\n",
    "#onnxruntime es una librería de ejecución de modelos ONNX (Open Neural Network Exchange) que permite cargar y\n",
    "# ejecutar modelos ONNX en diferentes plataformas y dispositivos.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from xgboost import XGBClassifier, XGBRegressor, DMatrix, train as train_xgb\n",
    "\n",
    "#pip install --user skl2onnx -- Instala esta libreria como usuario, para no tener problemas de acceso\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import convert_sklearn, to_onnx, update_registered_converter\n",
    "from skl2onnx.common.shape_calculator import (\n",
    "    calculate_linear_classifier_output_shapes,\n",
    "    calculate_linear_regressor_output_shapes,\n",
    ")\n",
    "from onnxmltools.convert import convert_xgboost as convert_xgboost_booster\n",
    "from onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost\n",
    "#skl2onnx es una librería que proporciona herramientas para convertir modelos sklearn a \n",
    "#formato ONNX. onnxmltools es otra librería que proporciona herramientas para convertir modelos xgboost a formato ONNX. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Registro del convertidor XGB CLASIFICADOR\n",
    "# update_registered_converter(\n",
    "#     XGBClassifier,\n",
    "#     \"XGBoostXGBClassifier\",\n",
    "#     calculate_linear_classifier_output_shapes,\n",
    "#     convert_xgboost,\n",
    "#     options={\"nocl\": [True, False], \"zipmap\": [True, False, \"columns\"]},\n",
    "# )\n",
    "\n",
    "# Registro del convertidor XGB REGRESSOR\n",
    "update_registered_converter(\n",
    "    xgb.XGBRegressor,\n",
    "    \"XGBoostXGBRegressor\",\n",
    "    calculate_linear_regressor_output_shapes,\n",
    "    convert_xgboost\n",
    ")\n",
    "\n",
    "# Supongamos que tienes un DataFrame llamado df con tus datos, y la columna objetivo es 'target'\n",
    "# Asegúrate de tener las características adecuadas para entrenar el modelo\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Crear el modelo de XGBoost\n",
    "model_pipe_xgb = Pipeline([(\"xgb\", xgb.XGBRegressor(\n",
    "    n_estimators=300,  # Número de árboles\n",
    "    max_depth=15,  # Profundidad máxima de los árboles\n",
    "    learning_rate=0.03, # tasa de aprendizaje, mas lento en ajustarse, pero necesitarias mas arboles\n",
    "    subsample = 0.7,# Proporción de entrenamiento utilizadas cada árbol\n",
    "    #reg_alpha = 0.5, # regularizacion L1 Lasso (restringe a 0)\n",
    "    reg_lambda = 2.5, # regulazirzacion L2 Ridge (restringe al minimo)\n",
    "    random_state=42 #Fijar la aleatoriedad durante el entrenamiento \n",
    "))])\n",
    "\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_pipe_xgb.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "# Realizar predicciones en el conjunto de train\n",
    "y_pred_xgb_train = model_pipe_xgb.predict(X_train_xgb)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_xgb_train = np.round(y_pred_xgb_train).astype('int64')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred_xgb_test = model_pipe_xgb.predict(X_test_xgb)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_xgb_test = np.round(y_pred_xgb_test).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb1f1f66-8570-4809-b1b4-7f9355817e63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Medir el rendimiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc9971b-c743-40c5-b4d7-034781c750b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Medir el rendimiento del modelo\n",
    "# error_absoluto_medio_dt = metrics.mean_absolute_error(y_test_dt, y_pred_dt)\n",
    "# error_cuadratico_medio_dt = metrics.mean_squared_error(y_test_dt, y_pred_dt)\n",
    "raiz_error_cuadratico_medio_xgb_train = metrics.mean_squared_error(y_train_xgb, y_pred_xgb_train, squared=False) #squared=False indica a la #función que calcule la raíz cuadrada \n",
    "raiz_error_cuadratico_medio_xgb_test = metrics.mean_squared_error(y_test_xgb, y_pred_xgb_test, squared=False)\n",
    "\n",
    "# Imprimir métricas de rendimiento\n",
    "# print(f'Error Absoluto Medio: {error_absoluto_medio_dt}')\n",
    "# print(f'Error Cuadrático Medio: {error_cuadratico_medio_dt}')\n",
    "print(f'Raíz del Error Cuadrático Medio Train: {raiz_error_cuadratico_medio_xgb_train}')\n",
    "print(f'Raíz del Error Cuadrático Medio Test: {raiz_error_cuadratico_medio_xgb_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e288ec1c-25c1-4d98-89a3-427f46562563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Convertir el Modelo Pipe a ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2420147b-47bd-48ff-8d37-5dc01c994728",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_onnx = convert_sklearn(\n",
    "    model_pipe_xgb,\n",
    "    \"xgboost_v1\",\n",
    "    [(\"input\", FloatTensorType([None, X.shape[1]]))],\n",
    "    target_opset={\"\": 12, \"ai.onnx.ml\": 2},\n",
    ")\n",
    "\n",
    "# And save.\n",
    "with open(\"xgboost_v1.onnx\", \"wb\") as f:\n",
    "    f.write(model_onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07bf19ec-2ff7-4398-9bf7-7ca8ea339af4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hacer una prediccion (Inferencia) con el Modelo ONNX convertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0869cc1f-4ef6-4d26-9a29-267014fd6f60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8669c934-0b86-4bd6-9ab4-03be0d1325c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"xgboost_v1.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "pred_onx = sess.run(None, {\"input\": X[:2].astype(np.float32)})\n",
    "print(\"predict\", pred_onx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e15fa053-daca-4819-941e-9498cff2b83a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from onnxmltools.convert import convert_xgboost as convert_xgboost_booster\n",
    "# import onnxmltools\n",
    "\n",
    "# # Convertir el modelo XGBoost a ONNX\n",
    "# onnx_model = convert_xgboost_booster(model_xgb, \n",
    "#                                      'XGBoost model', \n",
    "#                                      initial_types=[('input',\n",
    "#                                                       FloatTensorType([None, X.shape[1]]))])\n",
    "\n",
    "# # Guardar el modelo ONNX en un archivo\n",
    "# onnx_file = \"xgboost_model.onnx\"\n",
    "# onnxmltools.utils.save_model(onnx_model, onnx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fabf330-ef1e-421f-9063-5eadc773464c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Extraer mediante el metodo de envoltura las \"Variables mas importantes para el modelo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf13c9f-b572-4a05-aba6-f6b2933b4af5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener las importancias de las características\n",
    "feature_importances_xgb = model_xgb.feature_importances_\n",
    "\n",
    "# Crear un DataFrame para mostrar las importancias de las características junto con sus nombres\n",
    "feature_importance_df_xgb = pd.DataFrame({\n",
    "    'Feature': datos_total_escalados_xgb.columns,\n",
    "    'Importance': feature_importances_xgb\n",
    "})\n",
    "\n",
    "# Ordenar el DataFrame por importancia en orden descendente\n",
    "feature_importance_df_xgb = feature_importance_df_xgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 10 características más importantes\n",
    "print(feature_importance_df_xgb.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07d214a-4909-42be-9ee6-0f7e5006b44d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener las importancias de las características\n",
    "feature_importances_xgb = model_xgb.feature_importances_\n",
    "\n",
    "# Crear un DataFrame para mostrar las importancias de las características junto con sus nombres\n",
    "feature_importance_df_xgb = pd.DataFrame({\n",
    "    'Feature': datos_total_escalados.columns,\n",
    "    'Importance': feature_importances_xgb\n",
    "})\n",
    "\n",
    "# Ordenar el DataFrame por importancia en orden descendente\n",
    "feature_importance_df_xgb = feature_importance_df_xgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 10 características más importantes\n",
    "print(feature_importance_df_xgb.head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b79c44b-1b3d-4cf6-84f8-578c64f1d5a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tunnig de Hiperparametros del Modelo con GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de16f11-6ff2-4309-8895-b14e3aab1197",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Definir los parámetros para la búsqueda\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  #Numero  de arboles que entrenaran\n",
    "    'max_depth': [10, 15, 20],   #profundidad de cada arbol \n",
    "    'learning_rate': [0.01], #tasa de aprendizaje, valor más bajo significa que el modelo se ajusta más lentamente,que puede resultar un mejor rendimiento pero requiere un número mayor de árboles\n",
    "    #'subsample': [0.7],  #Proporción de las muestras de entrenamiento utilizadas para construir cada árbol. Valores menores a 1.0 pueden prevenir el sobreajuste al introducir aleatoriedad  \n",
    "    #'reg_alpha': [0, 0.1, 0.5]  #Regularización L1 (alpha o Lasso): Útil cuando se desea un modelo más simple y se sospecha que muchas características no son relevantes. ()\n",
    "    #L1 Penaliza las características menos importantes y puede llevar algunas de ellas a tener un peso cero, esencialmente eliminándolas del modelo\n",
    "    \n",
    "    'reg_lambda': [1.5, 2],  # Regularización L2 (lambda o Ridge): Útil cuando se desea un modelo que sea robusto y evite el sobreajuste, pero sin eliminar completamente ninguna característica.\n",
    "    # Penaliza las características con grandes coeficientes y reduce el impacto de las características menos importantes, sin eliminarlas por completo\n",
    "}\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Realizar la búsqueda en la cuadrícula con validación cruzada\n",
    "grid_search = GridSearchCV(estimator=model_xgb, param_grid=param_grid, cv=2, n_jobs=-1, verbose=2)\n",
    "#n_jobs=-1, se utilizan todos los núcleos de CPU disponibles en la máquina. Esto permite maximizar el uso de recursos del sistema para acelerar el proceso.\n",
    "# validación cruzada (CV) divide el conjunto de datos en un número de particiones (folds), entrena el modelo en todas las particiones menos una y valida en la partición restante. Este proceso se repite para cada partición, asegurando que cada parte de los datos se utilice para validación exactamente una vez\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Imprimir los mejores parámetros y el mejor score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score found: \", grid_search.best_score_)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe41979b-1b54-44df-8822-57bc575703a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3631a167-9333-4e7a-85a1-e4955063c30e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.7 Red Neuronal MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7557ef12-b591-49fd-96dc-9257695493b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aseguramos que los resultados sean \"reproducibles\" en cada ejecucion de tensorflow(pesos iniciales aleatorios)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa8799c7-b309-4869-a69e-39676d7814e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suponiendo que tienes tus datos X e y definidos antes de esta sección\n",
    "\n",
    "\n",
    "#Dividir los sets de entrenamiento, validacion, test\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set (Dejas el 0.75 para entrenamiento 1-0.75 = 0.25 quedan libres)\n",
    "x_train_mlp, x_test_mlp, y_train_mlp, y_test_mlp = train_test_split(X, y, test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set(de lo que sobro arriba 25%, sacas tu validacion y test)\n",
    "x_val_mlp, x_test_mlp, y_val_mlp, y_test_mlp = train_test_split(x_test_mlp, y_test_mlp, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "# ---  test_ratio/(test_ratio + validation_ratio) ---\n",
    "#Esto significa que del 25% de datos restantes, el 40% será para el conjunto de prueba y el 60% para el conjunto de validación.\n",
    "#Con esta proporción:\n",
    "#x_val_lstm y y_val_lstm contendrán 0.60 * 0.25 = 0.15 del total original.\n",
    "#x_test_lstm y y_test_lstm contendrán 0.40 * 0.25 = 0.10 del total original.\n",
    "\n",
    "\n",
    "# Crear el modelo de la red neuronal\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(10, input_dim=X.shape[1], activation='relu')) #input_dim : numero de variables inputs\n",
    "model_mlp.add(Dense(20, activation='relu'))\n",
    "model_mlp.add(Dense(10, activation='relu'))\n",
    "model_mlp.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model_mlp.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_mlp.summary()\n",
    "\n",
    "# Entrenar el modelo y registrar las pérdidas\n",
    "# Entrenar el modelo y registrar las pérdidas\n",
    "history = model_mlp.fit(x_train_mlp, y_train_mlp, epochs=50, batch_size=5, verbose=2, validation_data=(x_val_mlp, y_val_mlp))\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred_mlp = model_mlp.predict(x_test_mlp)\n",
    "y_pred_mlp = np.round(y_pred_mlp).astype('int64')\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse = mean_squared_error(y_test_mlp, y_pred_mlp)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6749a7bc-5936-4bad-9976-483f72397a72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vizualizar las Perdidas(Para detectar si hay Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffab6948-2e86-4df1-96f9-20cec59f3a09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Graficar las pérdidas de entrenamiento y validación\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.title('Curva de aprendizaje')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e50ac87d-d891-4901-95de-bde66e7befeb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluar el rendimiento del Modelo en el Conj. de Entrenamiento y Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9d0382e-e6d2-49af-af1c-9a5aa87bf3cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hacer predicciones en el conjunto de ENTRENAMIENTO\n",
    "y_train_pred_mlp = model_mlp.predict(x_train_mlp)\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de entrenamiento\n",
    "mse_train = mean_squared_error(y_train_mlp, y_train_pred_mlp)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Hacer predicciones en el conjunto de VALIDACION\n",
    "y_pred_val_mlp = model_mlp.predict(x_val_mlp)\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse_val = mean_squared_error(y_val_mlp, y_pred_val_mlp)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Hacer predicciones en el conjunto de PRUEBA\n",
    "y_pred_mlp = model_mlp.predict(x_test_mlp)\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse_test = mean_squared_error(y_test_mlp, y_pred_mlp)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de entrenamiento: {rmse_train}')\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de entrenamiento: {rmse_val}')\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1790b5fc-b1e7-441f-b661-efa7276c1784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Feature Importances in REDES NEURONALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ec85337-2184-473f-9ef4-4ba2bc4b4adf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metodo 1: Metodo Basado en Gradientes\n",
    "- Gradiente: En términos matemáticos, el gradiente es una medida de cuánto cambia la salida de una función (en este caso, la red neuronal) en respuesta a cambios en sus entradas. Es un vector que apunta en la dirección del mayor aumento de la función.\n",
    "\n",
    "- Importancia de las características: La importancia de una característica se puede estimar observando la magnitud del gradiente de la salida con respecto a esa característica. Una mayor magnitud del gradiente sugiere que pequeños cambios en esa característica tienen un gran impacto en la salida, lo que indica una mayor importancia.\n",
    "\n",
    "- Calcular los gradientes: Una vez que la red neuronal está entrenada, se calculan los gradientes de la salida de la red con respecto a cada característica de entrada. Esto se hace usando el cálculo automático de derivadas, que es una capacidad estándar en bibliotecas de aprendizaje profundo como TensorFlow y PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e098663d-c4c9-4e0d-b651-68c755383d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('your_model.h5') # Load your trained model\n",
    "\n",
    "# X = np.load('your_input_data.npy') # Load your input data\n",
    "\n",
    "# Convertir X(datos de caracteristicas MLP Red Neuronal Multicapa) a tensor de TensorFlow\n",
    "input_tensor = tf.convert_to_tensor(X)\n",
    "\n",
    "# Calcular gradientes de la salida con respecto a las entradas\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_tensor)\n",
    "    output = model_mlp(input_tensor)\n",
    "\n",
    "gradients = tape.gradient(output, input_tensor)\n",
    "\n",
    "# Calcular la importancia de las características como la media absoluta de los gradientes\n",
    "feature_importance = np.mean(np.abs(gradients.numpy()), axis=0)\n",
    "\n",
    "# Obtener los nombres de las características del DataFrame original\n",
    "feature_names = datos_total_escalados_mlp.columns\n",
    "\n",
    "# Crear un DataFrame para asociar nombres de características con sus importancias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cba7daf-2d5c-4686-8017-fc7866477f03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metodo 2:  basado en pesos (weight-based methods) \n",
    "- se utiliza para estimar la importancia de las características en una red neuronal considerando los pesos o conexiones entre las neuronas\n",
    "- Extraer Pesos: Obtener los pesos de las capas del modelo, especialmente de la primera capa si se está evaluando la importancia de las características de entrada.\n",
    "-Calcular Importancia: Calcular la importancia de cada característica sumando los valores absolutos de sus pesos. Esto se basa en la idea de que los pesos con valores absolutos mayores tienen un impacto mayor en la salida del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbb144d2-86fa-45a6-bbf7-c943c399af18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los pesos de las capas del modelo\n",
    "weights = model_mlp.get_weights()\n",
    "\n",
    "# Calcular la importancia de las características como la suma de los valores absolutos de los pesos de la primera capa\n",
    "feature_importance = np.sum(np.abs(weights[0]), axis=1)\n",
    "\n",
    "# Obtener los nombres de las características del DataFrame original\n",
    "feature_names = datos_total_escalados_mlp.columns\n",
    "\n",
    "# Crear un DataFrame para asociar nombres de características con sus importancias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Importancia de las características:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e24979-7c8a-4189-9de9-15b878626743",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Convertir el Modelo a Tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b17c068-d0e5-4700-9321-c48c47d909de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Convertir desde la misma sesion y Configurar el convertidor TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45121326-4a51-4ed5-8cc4-944760c83709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir desde la misma sesion y Configurar el convertidor TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model=model_mlp)\n",
    "\n",
    "# Especificar las operaciones admitidas por el modelo TFLite\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# Deshabilitar la conversión de operaciones de lista de tensores\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convertir el modelo a TensorFlow Lite\n",
    "model_tflite = converter.convert()\n",
    "\n",
    "# Guardar el modelo TensorFlow Lite en disco\n",
    "with open(\"model_mlp_v1.tflite\", \"wb\") as f:\n",
    "    f.write(model_tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50bd9cde-d650-4134-8e90-1a5c7fb33ea8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Convertir desde Cargar el modelo desde la ubicacion donde se guardo con \"SavedModel de TensorFlow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77c538a-0a2c-4f0b-ad90-d1968a2f0583",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Guardar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7cf640-a141-4e0c-9a9d-a89892c36527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "\n",
    "# Guardando todos los componentes del modelo (arquitectura, pesos, configuración de entrenamiento, estado del optimizador, etc.)\n",
    "# SavedModel de TensorFlow\n",
    "model_mlp.save(\"model_mlp_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f22ab91-2892-4375-af06-d10e530d7bc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Cargar el modelo desde la ubicacion donde se guardo con \"SavedModel de TensorFlow\"\n",
    "model_cargado = tf.keras.models.load_model('model_mlp_v1.h5')\n",
    "\n",
    "# Convertir el modelo a formato .tflite\n",
    "converter2 = tf.lite.TFLiteConverter.from_keras_model(model_cargado)\n",
    "\n",
    "# Especificar el conjunto de operaciones admitidas\n",
    "converter2.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Operaciones TensorFlow Lite estándar (mayoría de los modelos, como convoluciones, activaciones, etc.)\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS      # Operaciones TensorFlow adicionales - Esto debes instalar en el Android Studio Gradle\n",
    "]\n",
    "\n",
    "# Convertir el modelo\n",
    "tflite_model2 = converter2.convert()\n",
    "\n",
    "# Guardar el modelo convertido en formato .tflite\n",
    "with open('model_mlp_v1_2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "810a195e-24ea-4214-a270-17055b4d16b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tunnig de Hiperparametros del Modelo de AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20ad2617-20f1-4bfe-9b4d-a1cfaa94d34f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1.Metodo de Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b770e02d-1c50-4bc8-8033-59fb34b9d98d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Suponiendo que tienes tus datos X e y definidos antes de esta sección\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear una función para construir el modelo\n",
    "def create_model(optimizer='adam'):\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(Dense(10, input_dim=5, activation='relu'))\n",
    "    model_rnn.add(Dense(20, activation='relu'))\n",
    "    model_rnn.add(Dense(10, activation='relu'))\n",
    "    model_rnn.add(Dense(1, activation='linear'))\n",
    "    model_rnn.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model_rnn\n",
    "\n",
    "# Crear el KerasRegressor\n",
    "model_rnn = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Definir la búsqueda en cuadrícula de hiperparámetros\n",
    "param_grid = {\n",
    "    'batch_size': [10, 20],\n",
    "    'epochs': [20, 40, 60, 100],\n",
    "    'optimizer': ['adam', 'rmsprop']\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda en cuadrícula\n",
    "grid = GridSearchCV(estimator=model_rnn, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_result = grid.fit(X_train_rnn, y_train_rnn)\n",
    "\n",
    "# Resumir los resultados\n",
    "print(f\"Mejor puntuación (neg_mean_squared_error): {grid_result.best_score_}\")\n",
    "print(f\"Mejores hiperparámetros: {grid_result.best_params_}\")\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred_rnn = best_model.predict(X_test_rnn)\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse = mean_squared_error(y_test_rnn, y_pred_rnn)\n",
    "# Calcular la raíz cuadrada del error cuadrático medio\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse}')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(grid_result.best_estimator_.model.history.history['loss'])\n",
    "plt.plot(grid_result.best_estimator_.model.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccacad44-544c-4c6d-b91b-705500c50d6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.8 Red Neuronal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef46190-76cb-471f-a655-3ef609baf2ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Luego de Hacer el Feature Importances (Metodo de Envoltura) se identifico este conjunto de variables mas importantes\n",
    "datos_total_escalados_lstm =  datos_total_escalados[['tonelaje_inicial_poligono_creado',\n",
    "'elevacion_poligono_metros',\n",
    "'CF02','CF01', 'CF03', 'PH03', 'PH02', 'PH01', 'FR01',\n",
    "'tiempo_ready_llegada_esperando',\n",
    "'cantidad_equipos_espera_al_termino_carga_pala',\n",
    "'densidad_inicial_poligono_creado_tn/m3']]\n",
    "\n",
    "# Supongamos que datos es tu DataFrame y has seleccionado tus características (X) y variable objetivo (y)\n",
    "# Capacidad_peso = capacidad_volumen * densidad   O  capacidad_volumen = Capacidad_peso / densidad\n",
    "X = datos_total_escalados_lstm.values #Sale de los datos escalados\n",
    "\n",
    "y = datos['numero_pases_carguio'].values # Reemplaza 'variable_objetivo' con el nombre de tu variable objetivo(sale de los datos originales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0407a1a7-d5c8-4a39-819f-c1561b99f0f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9baa31b1-a04f-4df0-b5c0-137a2a23bff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# En LSTM, debes mantener la secuencia al dividir en Train, Validation, Test\n",
    "\n",
    "# Definir los porcentajes para cada conjunto (ajustados según tu necesidad)\n",
    "train_ratio = 0.75  # 75% para entrenamiento\n",
    "validation_ratio = 0.15  # 15% para validación\n",
    "test_ratio = 0.10  # 10% para prueba\n",
    "\n",
    "# Calcular los índices para dividir los datos\n",
    "num_samples = len(X)\n",
    "train_idx = int(train_ratio * num_samples)\n",
    "val_idx = int((train_ratio + validation_ratio) * num_samples)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
    "x_train_lstm, y_train_lstm = X[:train_idx], y[:train_idx]\n",
    "x_val_lstm, y_val_lstm = X[train_idx:val_idx], y[train_idx:val_idx]\n",
    "x_test_lstm, y_test_lstm = X[val_idx:], y[val_idx:]\n",
    "\n",
    "\n",
    "# Crear el modelo LSTM\n",
    "model_lstm = Sequential()\n",
    "#Cada unidad LSTM calcula y actualiza su estado interno basado en la entrada actual, el estado anterior y una compuerta de olvido que controla cuánta #información pasa a través del tiempo.\n",
    "model_lstm.add(LSTM(30, activation='relu', input_shape=(1, X.shape[1]))) # X.shape[1] : Numero de variables input\n",
    "model_lstm.add(Dense(50, activation='relu'))  # ReLU(x)=max(0,x)\n",
    "#ReLU introduce no linealidad en el modelo, Sin esta no linealidad, las redes neuronales se reducirían a combinaciones lineales de sus entradas\n",
    "# ReLU Al permitir que las activaciones positivas pasen sin cambio y descartar las negativas, ReLU facilita que la red neuronal aprenda representaciones más #discriminativas de los datos\n",
    "model_lstm.add(Dense(30, activation='relu'))\n",
    "model_lstm.add(Dense(15, activation='relu'))\n",
    "model_lstm.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "\n",
    "# Entrenar el modelo y registrar las pérdidas input:(numero_muestras, bacthes, num_variables), output:(numero_muestras, bacthes, num_variables)\n",
    "history = model_lstm.fit(x_train_lstm.reshape(-1, 1, X.shape[1]), y_train_lstm, epochs=50, batch_size=5, verbose=2, validation_data=(x_val_lstm.reshape(-1, 1, X.shape[1]), y_val_lstm))\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred_lstm = model_lstm.predict(x_test_lstm.reshape(-1, 1, X.shape[1]))\n",
    "y_pred_lstm = np.round(y_pred_lstm).astype('int64')\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse = mean_squared_error(y_test_lstm, y_pred_lstm)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3407c33-3c0d-452b-9cf0-4cfb7a67d6ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vizualizar las Perdidas(Para detectar si hay Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c19a6d-8d82-47b2-8c09-65a38c121305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Graficar las pérdidas de entrenamiento y validación\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.title('Curva de aprendizaje LSTM')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1897c47-3142-4fc4-9f65-1ccb47150fd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluar el rendimiento del Modelo en el Conj. de Entrenamiento y Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdf3749-ab32-46f5-99a1-78947b9bed0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_test_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342972c5-8306-406a-a88e-c737feef5e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hacer predicciones en el conjunto de ENTRENAMIENTO\n",
    "y_train_pred_lstm = model_lstm.predict(x_train_lstm.reshape(-1, 1, X.shape[1]))\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de entrenamiento\n",
    "mse_train_lstm = mean_squared_error(y_train_lstm, y_train_pred_lstm)\n",
    "rmse_train_lstm = np.sqrt(mse_train_lstm)\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # Hacer predicciones en el conjunto de VALIDACION\n",
    "y_val_pred_lstm = model_lstm.predict(x_val_lstm.reshape(-1, 1, X.shape[1]))\n",
    "\n",
    "# # Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse_val_lstm = mean_squared_error(y_val_lstm, y_val_pred_lstm)\n",
    "rmse_val_lstm = np.sqrt(mse_val_lstm)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Hacer predicciones en el conjunto de PRUEBA\n",
    "y_test_pred_lstm = model_lstm.predict(x_test_lstm.reshape(-1, 1, X.shape[1]))\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse_test_lstm = mean_squared_error(y_test_lstm, y_test_pred_lstm)\n",
    "rmse_test_lstm = np.sqrt(mse_test_lstm)\n",
    "\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de entrenamiento: {rmse_train_lstm}')\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de entrenamiento: {rmse_val_lstm}')\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse_test_lstm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9490ac43-c405-487f-b855-c01df528a778",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Feature Importances in REDES NEURONALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ae272c-2c82-44be-b32b-56fc47b68ed9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metodo 1: Metodo Basado en Gradientes\n",
    "- Gradiente: En términos matemáticos, el gradiente es una medida de cuánto cambia la salida de una función (en este caso, la red neuronal) en respuesta a cambios en sus entradas. Es un vector que apunta en la dirección del mayor aumento de la función.\n",
    "\n",
    "- Importancia de las características: La importancia de una característica se puede estimar observando la magnitud del gradiente de la salida con respecto a esa característica. Una mayor magnitud del gradiente sugiere que pequeños cambios en esa característica tienen un gran impacto en la salida, lo que indica una mayor importancia.\n",
    "\n",
    "- Calcular los gradientes: Una vez que la red neuronal está entrenada, se calculan los gradientes de la salida de la red con respecto a cada característica de entrada. Esto se hace usando el cálculo automático de derivadas, que es una capacidad estándar en bibliotecas de aprendizaje profundo como TensorFlow y PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17e35ef-f177-4b5f-a095-3215df3c0710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('your_model.h5') # Load your trained model\n",
    "\n",
    "# X = np.load('your_input_data.npy') # Load your input data\n",
    "\n",
    "# Expandir la dimensión de los datos de entrada para que sean compatibles con el modelo LSTM\n",
    "#X_expanded = np.expand_dims(X, axis=1)  # (330118, 26) -> (330118, 1, 26)\n",
    "X_reshape = X.reshape(-1, 1, X.shape[1])       # (330118, 26) -> (330118, 1, 26), X.shape[1] : Numero variables del modelo\n",
    "# X_reshape.shape\n",
    "\n",
    "# Convertir X(datos de caracteristicas LSTM) a tensor de TensorFlow\n",
    "input_tensor = tf.convert_to_tensor(X_reshape)\n",
    "\n",
    "# Calcular gradientes de la salida con respecto a las entradas\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_tensor)\n",
    "    output = model_lstm(input_tensor)\n",
    "\n",
    "gradients = tape.gradient(output, input_tensor)\n",
    "\n",
    "# Reducir la dimensión del tensor de gradientes\n",
    "gradients = tf.squeeze(gradients, axis=1)  # (330118, 1, 26) -> (330118, 26)\n",
    "\n",
    "# Calcular la importancia de las características como la media absoluta de los gradientes\n",
    "feature_importance = np.mean(np.abs(gradients.numpy()), axis=0)\n",
    "\n",
    "# Obtener los nombres de las características del DataFrame original\n",
    "feature_names = datos_total_escalados_lstm.columns\n",
    "\n",
    "# Crear un DataFrame para asociar nombres de características con sus importancias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3e67db-a3f8-4f4d-919c-6cc0e65aad6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metodo 2:  basado en pesos (weight-based methods) \n",
    "- se utiliza para estimar la importancia de las características en una red neuronal considerando los pesos o conexiones entre las neuronas\n",
    "- Extraer Pesos: Obtener los pesos de las capas del modelo, especialmente de la primera capa si se está evaluando la importancia de las características de entrada.\n",
    "-Calcular Importancia: Calcular la importancia de cada característica sumando los valores absolutos de sus pesos. Esto se basa en la idea de que los pesos con valores absolutos mayores tienen un impacto mayor en la salida del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f58dc73-e109-4d32-8a57-e9d077f87b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los pesos de las capas del modelo\n",
    "weights = model_lstm.get_weights()\n",
    "\n",
    "# Calcular la importancia de las características como la suma de los valores absolutos de los pesos de la primera capa\n",
    "feature_importance = np.sum(np.abs(weights[0]), axis=1)\n",
    "\n",
    "# Obtener los nombres de las características del DataFrame original\n",
    "feature_names = datos_total_escalados_lstm.columns\n",
    "\n",
    "# Crear un DataFrame para asociar nombres de características con sus importancias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Importancia de las características:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf0a91bd-a77b-45d1-8df2-b5ec9248714b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Metodo 3: Extracción y Procesamiento de Pesos de Todas las Capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435a4637-7174-4288-845e-5ebab5f4b1b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# # Cargar el modelo previamente entrenado\n",
    "# model = tf.keras.models.load_model('your_model.h5')\n",
    "\n",
    "# Obtener los pesos de las capas del modelo\n",
    "weights = model_rnn.get_weights()\n",
    "\n",
    "# Inicializar una lista para almacenar la importancia de cada característica\n",
    "num_features = weights[0].shape[0]\n",
    "total_importance = np.zeros(num_features)\n",
    "\n",
    "# Calcular la importancia de cada característica a través de todas las capas\n",
    "for i in range(0, len(weights), 2):  # Saltamos cada 2 para solo tomar los pesos y no los biases\n",
    "    layer_weights = weights[i]\n",
    "    if len(layer_weights.shape) == 2:  # Asegurar que es una capa densamente conectada\n",
    "        layer_importance = np.sum(np.abs(layer_weights), axis=1)\n",
    "        total_importance += layer_importance\n",
    "\n",
    "# Obtener los nombres de las características del DataFrame original\n",
    "feature_names = datos_total_escalados.columns\n",
    "\n",
    "# Crear un DataFrame para asociar nombres de características con sus importancias\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': total_importance\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Importancia de las características:\")\n",
    "print(importance_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bb2eb49-cf35-4eea-a877-7e46fba25986",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Convertir el Modelo a Tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2904764-249e-4546-934a-e26827def3dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Convertir desde la misma sesion y Configurar el convertidor TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a65e8d-2aa5-4aa6-8759-9d035e7599eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir desde la misma sesion y Configurar el convertidor TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model=model_lstm)\n",
    "\n",
    "# Especificar las operaciones admitidas por el modelo TFLite\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# Deshabilitar la conversión de operaciones de lista de tensores\n",
    "#(converter._experimental_lower_tensor_list_ops = True): Significa que el convertidor intentará simplificar las operaciones de listas de tensores a operaciones más primitivas y compatibles con TensorFlow Lite (TFLite).\n",
    "#(converter._experimental_lower_tensor_list_ops = False): Significa que el convertidor no realizará esta simplificación, y las operaciones de listas de tensores se mantendrán en su forma original, lo cual puede resultar en incompatibilidades con TFLite.\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convertir el modelo a TensorFlow Lite\n",
    "model_tflite = converter.convert()\n",
    "\n",
    "# Guardar el modelo TensorFlow Lite en disco\n",
    "with open(\"model_lstm_v1.tflite\", \"wb\") as f:\n",
    "    f.write(model_tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea8dfb47-89be-4f65-9d5c-ea2792da0c35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Convertir desde Cargar el modelo desde la ubicacion donde se guardo con \"SavedModel de TensorFlow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d58edaa0-5370-44e9-ae12-19489a9256a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Guardar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bd9803-04f2-4852-bb69-bd832be417a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "\n",
    "# Guardando todos los componentes del modelo (arquitectura, pesos, configuración de entrenamiento, estado del optimizador, etc.)\n",
    "# SavedModel de TensorFlow\n",
    "model_lstm.save(\"model_lstm_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e956ae8-a576-49c6-ae65-665f07d0f0c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Cargar el modelo desde la ubicacion donde se guardo con \"SavedModel de TensorFlow\"\n",
    "model_cargado = tf.keras.models.load_model('model_mlp_v1.h5')\n",
    "\n",
    "# Convertir el modelo a formato .tflite\n",
    "converter2 = tf.lite.TFLiteConverter.from_keras_model(model_cargado)\n",
    "\n",
    "# Especificar el conjunto de operaciones admitidas\n",
    "converter2.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Operaciones TensorFlow Lite estándar (mayoría de los modelos, como convoluciones, activaciones, etc.)\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS      # Operaciones TensorFlow adicionales - Esto debes instalar en el Android Studio Gradle\n",
    "]\n",
    "\n",
    "#(converter._experimental_lower_tensor_list_ops = True): Significa que el convertidor intentará simplificar las operaciones de listas de tensores a operaciones más primitivas y compatibles con TensorFlow Lite (TFLite).\n",
    "#(converter._experimental_lower_tensor_list_ops = False): Significa que el convertidor no realizará esta simplificación, y las operaciones de listas de tensores se mantendrán en su forma original, lo cual puede resultar en incompatibilidades con TFLite.\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convertir el modelo\n",
    "tflite_model2 = converter2.convert()\n",
    "\n",
    "# Guardar el modelo convertido en formato .tflite\n",
    "with open('model_mlp_v1_2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efeca91a-8dbb-4a3b-9d69-72b841afef63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5056c56-db57-4a86-b75f-523a63802f78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 6. Armando los BaseFinal, con los valores predichos de cada Modelo, y tambien medir el impacto con productividades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed72472f-bfb1-4294-a231-ba8f117be92f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agregar las predicciones como una columna adicional al conjunto de prueba\n",
    "df_resultados_models = combined_df.copy()\n",
    "df_resultados_models['Numero_pases_real'] = y_test_gen\n",
    "df_resultados_models['Numero_pases_reglineal'] = numero_pases_rl\n",
    "df_resultados_models['Numero_pases_arboldecision'] = numero_pases_dt\n",
    "df_resultados_models['Numero_pases_rforest'] = numero_pases_rf\n",
    "df_resultados_models['Numero_pases_xgb'] = numero_pases_xgb\n",
    "df_resultados_models['Numero_pases_mlp'] = numero_pases_mlp\n",
    "df_resultados_models['Numero_pases_lstm'] = numero_pases_lstm\n",
    "df_resultados_models['capacidad_en_peso_equipo_carguio'] = datos['capacidad_en_peso_equipo_carguio']\n",
    "df_resultados_models['tonelaje_vims_all'] = datos['tonelaje_segun_computadora']\n",
    "df_resultados_models.head()\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Regresion Lineal\n",
    "#df_resultados_models['Diferencia_Pases_rl'] = df_resultados_models['Numero_pases_reglineal'] - df_resultados_models['numero_pases_carguio']\n",
    "df_resultados_models['tonelaje_segun_computadora_rl'] = (df_resultados_models['tonelaje_vims_all']) + (df_resultados_models['Diferencia_Pases_rl']*df_resultados_models['capacidad_en_peso_equipo_carguio'])\n",
    "\n",
    "\n",
    "#Arbol de decision\n",
    "df_resultados_models['Diferencia_Pases_dt'] = df_resultados_models['Numero_pases_arboldecision'] - df_resultados_models['numero_pases_carguio']\n",
    "df_resultados_models['tonelaje_segun_computadora_dt'] = (df_resultados_models['tonelaje_vims_all']) + (df_resultados_models['Diferencia_Pases_dt']*df_resultados_models['capacidad_en_peso_equipo_carguio'])\n",
    "\n",
    "\n",
    "# Random Forest \n",
    "df_resultados_models['Diferencia_Pases_rf'] = df_resultados_models['Numero_pases_rforest'] - df_resultados_models['numero_pases_carguio']\n",
    "df_resultados_models['tonelaje_segun_computadora_rf'] = (df_resultados_models['tonelaje_vims_all']) + (df_resultados_models['Diferencia_Pases_rf']*df_resultados_models['capacidad_en_peso_equipo_carguio'])\n",
    "\n",
    "\n",
    "# XGB (eXtreme Gradient Boosting)\n",
    "df_resultados_models['Diferencia_Pases_xgb'] = df_resultados_models['Numero_pases_xgb'] - df_resultados_models['numero_pases_carguio']\n",
    "df_resultados_models['tonelaje_segun_computadora_xgb'] = (df_resultados_models['tonelaje_vims_all']) + (df_resultados_models['Diferencia_Pases_xgb']*df_resultados_models['capacidad_en_peso_equipo_carguio'])\n",
    "\n",
    "# RNN (Redes Neuronales MLP)\n",
    "df_resultados_models['Diferencia_Pases_MLP'] = df_resultados_models['Numero_pases_mlp'] - df_resultados_models['numero_pases_carguio']\n",
    "df_resultados_models['tonelaje_segun_computadora_MLP'] = (df_resultados_models['tonelaje_vims_all']) + (df_resultados_models['Diferencia_Pases_MLP']*df_resultados_models['capacidad_en_peso_equipo_carguio'])\n",
    "\n",
    "# RNN (Redes Neuronales LSTM)\n",
    "df_resultados_models['Diferencia_Pases_LSTM'] = df_resultados_models['Numero_pases_lstm'] - df_resultados_models['numero_pases_carguio']\n",
    "df_resultados_models['tonelaje_segun_computadora_LSTM'] = (df_resultados_models['tonelaje_vims_all']) + (df_resultados_models['Diferencia_Pases_LSTM']*df_resultados_models['capacidad_en_peso_equipo_carguio'])\n",
    "\n",
    "\n",
    "df_resultados_models.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e5b7a7f-b6b5-4623-afa7-d07c35116c33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 1. Train a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c276c2c-b965-4814-ba1c-f39e88032087",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MLflow Tracking\n",
    "[MLflow tracking](https://www.mlflow.org/docs/latest/tracking.html) allows you to organize your machine learning training code, parameters, and models. \n",
    "\n",
    "You can enable automatic MLflow tracking by using [*autologging*](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4386cf16-81d9-4744-ade2-c6fce039c92f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Establecer semillas en NumPy y TensorFlow\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52327a5a-c038-4aa0-b324-57be115e2a37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Modelo 1 - Redes LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3d9399-c2dd-4751-b3f6-b764be691e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias para redes neuronales(LSTM)\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Establecer semillas en NumPy y TensorFlow\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X_array, y_array, test_size=0.2, random_state=42)\n",
    "\n",
    "model_rnn = Sequential()\n",
    "# La forma de entrada para LSTM debe ser (n_timesteps, n_features)\n",
    "model_rnn.add(LSTM(30, activation='relu', input_shape=(1, 5)))  #  (n_samples, n_pasos, n_variables)\n",
    "# Agregar capas Dense según sea necesario\n",
    "model_rnn.add(Dense(60, activation='relu'))\n",
    "model_rnn.add(Dense(30, activation='relu'))\n",
    "model_rnn.add(Dense(15, activation='relu'))\n",
    "# Capa de salida\n",
    "model_rnn.add(Dense(1, activation='linear'))\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compilar el modelo\n",
    "model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# Imprimir un resumen del modelo\n",
    "model_rnn.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model_rnn.fit(X_train_rnn.reshape(-1, 1, 5), y_train_rnn, epochs=1, validation_data=(X_test_rnn.reshape(-1, 1, 5),y_test_rnn), batch_size=5, verbose=1)\n",
    "\n",
    "# VALIDACION 1\n",
    "# Obtener la pérdida en el conjunto de entrenamiento y el conjunto de validación\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Calcular el RMSE en el conjunto de entrenamiento y el conjunto de validación\n",
    "train_rmse = np.sqrt(train_loss)\n",
    "val_rmse = np.sqrt(val_loss)\n",
    "\n",
    "# VALIDACION 2\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred_rnn = model_rnn.predict(X_test_rnn.reshape(-1, 1, 5)) # (n_samples, n_pasos, n_variables)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_rnn = np.round(y_pred_rnn).astype('int64')\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse = mean_squared_error(y_test_rnn, y_pred_rnn)\n",
    "# Calcular la raíz cuadrada del error cuadrático medio\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8764ef5f-a1da-44a1-ad9f-81dd9fd0fd06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Establecer semillas en NumPy y TensorFlow\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#MLflow podra registrar automáticamente métricas, parámetros y el modelo generado durante el entrenamiento\n",
    "mlflow.tensorflow.autolog()  # Keras y TensorFlow\n",
    "\n",
    "# Entrenar el modelo\n",
    "with mlflow.start_run(run_name='experimento_MLOPS_V1'):  #Iniciliza un nuevo experimento de MlFlow\n",
    "    # Definir y compilar el modelo\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(LSTM(30, activation='relu', input_shape=(1, 5)))\n",
    "    model_rnn.add(Dense(60, activation='relu'))\n",
    "    model_rnn.add(Dense(30, activation='relu'))\n",
    "    model_rnn.add(Dense(15, activation='relu'))\n",
    "    model_rnn.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Entrenar el modelo y guardar el historial del entrenamiento\n",
    "    history = model_rnn.fit(X_train_rnn.reshape(-1, 1, 5), y_train_rnn, epochs=1, validation_data=(X_test_rnn.reshape(-1, 1, 5), y_test_rnn), batch_size=5, verbose=1)\n",
    "\n",
    "    # Save the run information to register the model later\n",
    "    #kerasURI = run.info.artifact_uri\n",
    "\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_params({\n",
    "        \"input_shape\": (1, 5),\n",
    "        \"lstm_units\": 30,\n",
    "        \"dense_layers\": [60, 30, 15],\n",
    "        \"activation\": \"relu\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss_function\": \"mean_squared_error\",\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 5\n",
    "    })\n",
    "\n",
    "    # Registrar la estructura del modelo en MLflow\n",
    "    #mlflow.keras.log_model(model_rnn, \"keras_tf_v1\")\n",
    "\n",
    "    # VALIDACION \n",
    "    # Obtener la pérdida en el conjunto de entrenamiento y el conjunto de validación\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "\n",
    "    # Calcular el RMSE en el conjunto de entrenamiento y el conjunto de validación\n",
    "    train_rmse = np.sqrt(train_loss)\n",
    "    test_rmse = np.sqrt(test_loss)\n",
    "\n",
    "    # Registro de métricas de RMSE\n",
    "    mlflow.log_metric(\"train_RMSE\", train_rmse[-1])  # Se registra el último valor de RMSE del conjunto de entrenamiento\n",
    "    mlflow.log_metric(\"test_RMSE\", test_rmse[-1])      # Se registra el último valor de RMSE del conjunto de validación\n",
    "    \n",
    "    # Crear gráfico de pérdida durante el entrenamiento\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.plot(epochs, train_loss, 'bo', label='Pérdida en entrenamiento')\n",
    "    plt.plot(epochs, test_loss, 'r', label='Pérdida en validación')\n",
    "    plt.title('Pérdida durante el entrenamiento y la validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"kerasplotv3.png\")\n",
    "    mlflow.log_artifact(\"kerasplotv3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f71112-0887-4995-8f0d-c0d4630afdfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a79b6da-c1f2-484c-bc22-3648669d9926",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Register to Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5389f1ed-220b-423b-b0bd-b5cafc7687b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Example_rnn\"                                  #nombre que se le dara al modelo que registraremos en el MlFlow Registry\n",
    "model_uri = f\"runs:/ed91e81d7ca644728f3e223fa3cfac92/model\" #identificador unico del experimento a registrar en Mlflow\n",
    "registered_model_version = mlflow.register_model(model_uri, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "619530ae-40b3-4869-98a7-127b2d1c330b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load from Model Registry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0c60a0-24ee-4d31-8071-a9e2d79c2070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "client = mlflow.tracking.MlflowClient() \n",
    "model_name = \"Example_rnn\"\n",
    "\n",
    "model_info = client.get_registered_model(model_name)\n",
    "model_version = model_info.latest_versions[0].version #Ultima version del modelo en MlFlow Registry\n",
    "#model_version = registered_model_version.version\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "# Supongamos que tienes tus datos de entrada en una lista\n",
    "input_data = np.array([[12, 35, 129, 48, 129]])\n",
    "\n",
    "# # Convertir los datos de entrada al tipo de datos float64\n",
    "input_data = input_data.astype(np.float64)\n",
    "\n",
    "# # Aumentamos a 3dimensiones los datos de entrada para que tengan la forma (None, 1, 5)\n",
    "input_data = np.reshape(input_data, (input_data.shape[0], 1, input_data.shape[1]))\n",
    "\n",
    "# # Realiza la predicción utilizando el modelo\n",
    "model.predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4f94afe-2d0f-44f6-bf78-295684c4fa64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load model without experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517634c5-0f13-41b3-b6a3-5c068cab81c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/ed91e81d7ca644728f3e223fa3cfac92/model\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c6fdb49-999a-4307-9118-0d9e9c4482d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Actualizar el modelo y tener una nueva version del Modelo de MlFlow Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd145bdb-01f7-4cfe-9b4b-fb47a7555f95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Definir el nombre del modelo y la nueva URI del modelo (para la versión 2)\n",
    "model_name = \"Example_rnn\"\n",
    "new_model_uri = 'runs:/3fdcef4bd83943ec9dc2188bc13045f4/model'  # Ejemplo de nueva URI del modelo\n",
    "\n",
    "# Registrar la nueva versión del modelo con la misma nombre pero con una nueva URI\n",
    "registered_model_version = mlflow.register_model(new_model_uri, model_name)\n",
    "\n",
    "# Si deseas, puedes obtener el ID de la nueva versión del modelo registrado\n",
    "new_version_id = registered_model_version.version\n",
    "new_version_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31f32055-1327-4ebf-8d14-da0d62db4686",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Validacion del Modelo antes de pasasr a Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7f866a-6e80-402a-9cd3-c772a3a4d917",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Obtener el modelo  in trancision, su nombre y version por el Webbook\n",
    "#model_name, model_version = fech_webbook_data()\n",
    "import mlflow\n",
    "# Interactuar con el servidor de MLflow\n",
    "client = mlflow.tracking.MlflowClient()  \n",
    "model_name = \"Example_rnn\"\n",
    "\n",
    "model_info = client.get_registered_model(model_name)\n",
    "model_version = model_info.latest_versions[0].version #Ultima version del modelo en MlFlow Registry\n",
    "\n",
    "models_details = client.get_model_version(model_name, model_version)\n",
    "run_info = client.get_run(run_id=models_details.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d968d96a-798a-472a-b561-dffe925f579e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39fcc6f2-bf68-44a7-a7ea-f97751d88083",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Validacion de la prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297157fb-3cf6-454f-90bb-7160a761cda8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89e0a88-eca8-4e5f-bb4f-9c99698f14b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "#Leer de Feature Store \n",
    "data_source = run_info.data.tags['db_table']\n",
    "features = fs.read_table(data_source)\n",
    "\n",
    "#Load model as a spark UDF\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri)\n",
    "\n",
    "#Selecciona las columnas de la tabla de caracteristicas, por el esquema input del modelo\n",
    "input_columns_names = loaded_model.metadata.get_input_schema().input_names()\n",
    "\n",
    "#Predict in spark dataframe\n",
    "try:\n",
    "    display(features.withColumn('predictions', loaded_model(*input_columns_names)))\n",
    "    client.set_model_version_tag(name=model_name, version=model_version, key=\"predicts\", value=1)\n",
    "except Exception:\n",
    "    print(\"No se puede predecir las caracteristicas,\")\n",
    "    client.set_model_version_tag(name=model_name, version=model_version, key=\"predicts\", value=0)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95839cf5-8303-4d88-93c8-5595520545f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Nombre del modelo registrado\n",
    "model_name = \"MLops_kerastf_v3\"\n",
    "\n",
    "# Crear una instancia del cliente de MLflow\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Obtener la información sobre el modelo\n",
    "model_info = client.get_registered_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4e0541-3bd6-4846-a3a7-60e8ca68376b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c799774-d3e0-484b-b4a9-ab141e1d3aa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estado al que deseas cambiar el modelo (p. ej., \"Production\", \"Staging\", \"Archived\")\n",
    "new_stage = \"None\"\n",
    "\n",
    "# Versión específica que deseas cambiar de estado\n",
    "#target_version = model_info.latest_versions[0].version # ultima version del modelo registrado en mlflow registry\n",
    "target_version = 1\n",
    "\n",
    "\n",
    "# Cambiar el estado del modelo\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version = target_version,  #\n",
    "    stage=new_stage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "000b9112-4079-480d-8f0b-d69fae5b8c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estado al que deseas cambiar el modelo (p. ej., \"Production\", \"Staging\", \"Archived\")\n",
    "new_stage = \"Production\"\n",
    "\n",
    "# Cambiar el estado del modelo\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_info.latest_versions[0].version,\n",
    "    stage=new_stage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ce886b-1c30-46b8-b132-11dc59e7f55c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Crear gráfico de pérdida durante el entrenamiento\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label='Pérdida en entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'r', label='Pérdida en validación')\n",
    "plt.title('Pérdida durante el entrenamiento y la validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3382b8d0-4ee8-47d8-8b6c-3ed5fd7a3ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Modelo 2 : Redes Tradicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9853087a-3f70-496b-8680-adb720259e22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias para redes neuronales(LSTM)\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 3. Construccion del modelo de RED NEURONAL\n",
    "# Aseguramos que los resultados sean \"reproducibles\" en cada ejecucion de tensorflow(pesos iniciales aleatorios)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Arquitectura 2:\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Dense(15, activation='relu', input_shape=[5]))\n",
    "model_rnn.add(Dense(7, activation='relu'))\n",
    "model_rnn.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# Imprimir un resumen del modelo\n",
    "model_rnn.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "# Red LSTM\n",
    "#history = model_rnn.fit(X_train_rnn.reshape(-1, 1, 5), y_train_rnn, epochs=5, validation_data=(X_test_rnn.reshape(-1, 1, 5),y_test_rnn), batch_size=5, verbose=1)\n",
    "# Red Tradicional\n",
    "history = model_rnn.fit(X_train_rnn, y_train_rnn, epochs=4, validation_data=(X_test_rnn,y_test_rnn), batch_size=5, verbose=1)\n",
    "\n",
    "# 4. Validacion del Modelo con la metrica de RSME\n",
    "# VALIDACION 1 del Modelo\n",
    "# Obtener la pérdida en el conjunto de entrenamiento y el conjunto de validación\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Calcular el RMSE en el conjunto de entrenamiento y el conjunto de validación\n",
    "train_rmse = np.sqrt(train_loss)\n",
    "val_rmse = np.sqrt(val_loss)\n",
    "\n",
    "# VALIDACION 2 del Modelo\n",
    "# Hacer predicciones en el conjunto de prueba Red LSTM\n",
    "#y_pred_rnn = model_rnn.predict(X_test_rnn.reshape(-1, 1, 5)) # (n_samples, n_pasos, n_variables)\n",
    "# Hacer predicciones en el conjunto de prueba Red Tradicional\n",
    "y_pred_rnn = model_rnn.predict(X_test_rnn) # (n_samples, n_pasos, n_variables)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_rnn = np.round(y_pred_rnn).astype('int64')\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse = mean_squared_error(y_test_rnn, y_pred_rnn)\n",
    "# Calcular la raíz cuadrada del error cuadrático medio\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d303d06d-4296-4cbf-9b94-c563c6d2b313",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Crear gráfico de pérdida durante el entrenamiento\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label='Pérdida en entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'r', label='Pérdida en validación')\n",
    "plt.title('Pérdida durante el entrenamiento y la validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532ddb61-3f2c-4052-87e3-24ef6df8efb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_pred_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b274f1da-e5a1-4bc8-90ce-f342a61bfd64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar un modelo en MLflow\n",
    "model_path = \"dbfs:/databricks/mlflow-tracking/8323b80e926241ee8763b7dc43085602/3fdcef4bd83943ec9dc2188bc13045f4/artifacts/model\"\n",
    "model_name = \"MLops_kerastf_v1\"\n",
    "mlflow.register_model(model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af3a01f-943e-4409-a614-592e3ecbd897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ver el directorio local de databrinks mas no el local de .os\n",
    "dbutils.fs.ls(\"dbfs:/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df2356b-8582-4906-b953-ea2bfe0cb6ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Guardar Modelo en Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621d2c95-ecd2-4658-9469-93f93af1af77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Guardar en el directorio LOCAL (.OS)\n",
    "model_rnn.save('/nuevomodelo/modelrnn_nuevo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "349ed379-807a-458e-b837-b4a138358ce4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Verificar que se ha guardado en el directorio LOCAL (.OS)\n",
    "import os\n",
    "os.listdir(\"/modelornn_version1_d4m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe3bae1-24bd-403d-814c-b33f777e6516",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Opcion 1: Cargar el Modelo ML directamente del Directorio (.OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2765c5c1-8d8e-4046-94d0-e98a024dd08b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo desde el contenedor de almacenamiento Azure\n",
    "loaded_model_nuevo = tf.keras.models.load_model(\"/nuevomodelo/modelrnn_nuevo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2638681c-a02a-426e-94a6-2f5f05ed0417",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Opcion  2: Copiar el Modelo ML desde el Directorio LOCAL(.OS)  HASTA  Directorio de DBFS de Azure DataBrinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6020d49-aa50-46a7-8d59-618603e2d66b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#copie los datos desde LOCAL(.OS) a DBFS como dbfs (DIRECTORIO DE DATABRINKS)\n",
    "dbutils.fs.cp(\"file:/nuevomodelo/modelrnn_nuevo\", \"dbfs:/nuevomodelo/modelrnn_nuevo\", recurse=True) # True: si copias una carpeta, False: si #solo es archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d7cef4-3479-4c9d-a1d8-674dfb543d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Verificar que se ha migrado desde Directorio LOCAL(.OS)  Hacia Directorio DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec38fcb3-0103-4181-b59e-303202ad9ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/nuevomodelo/modelrnn_nuevo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb1bdbeb-327b-41b5-b064-35f2e75a04a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Copiar el modelo desde el directorio DBFS al contenedor de almacenamiento Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7d3187-f0c7-498a-88a7-5960e258d7d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copiar el modelo desde el directorio DBFS al contenedor de almacenamiento Azure Storage\n",
    "dbutils.fs.cp(\"/nuevomodelo/modelrnn_nuevo\", \"/mnt/datalakemlopsd4m/presentation/modelrnn_nuevo\", recurse=True) #True:Copycarpeta,#False:Copyarchivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68462a76-164b-4caf-ae33-62dd7be082c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Establecer la URI de seguimiento de MLflow\n",
    "# mlflow.set_tracking_uri(\"databricks\")  # Usa \"databricks\" para Databricks\n",
    "\n",
    "# # Crear un nuevo experimento si no existe\n",
    "# experiment_name = \"nombre_del_experimento\"\n",
    "# mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Cargar los datos\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/presentation/datmarcobre_fengineer_tablacaract_delta\")\n",
    "datos = df_delta.toPandas()\n",
    "\n",
    "# Separar las variables independientes y dependiente\n",
    "X = datos[['capacidad_en_volumen_equipo_carguio_m3',\n",
    "           'capacidad_en_peso_equipo_carguio',\n",
    "           'capacidad_en_peso_equipo_acarreo',\n",
    "           'densidad_inicial_poligono_creado_tn/m3',\n",
    "           'capacidad_en_volumen_equipo_acarreo_m3']].values\n",
    "y = datos['numero_pases_carguio'].values\n",
    "\n",
    "# Construir el modelo de red neuronal\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mlflow.tensorflow.autolog()  # Keras y TensorFlow\n",
    "\n",
    "# Entrenar el modelo\n",
    "with mlflow.start_run(run_name='experimento_mlflow_keras_tf_3_21_24'):  #Iniciliza un nuevo experimento de MlFlow\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(LSTM(30, activation='relu', input_shape=(1, 5)))\n",
    "    model_rnn.add(Dense(60, activation='relu'))\n",
    "    model_rnn.add(Dense(30, activation='relu'))\n",
    "    model_rnn.add(Dense(15, activation='relu'))\n",
    "    model_rnn.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    history = model_rnn.fit(X_train_rnn.reshape(-1, 1, 5), y_train_rnn, epochs=1, \n",
    "                            validation_data=(X_test_rnn.reshape(-1, 1, 5), y_test_rnn), \n",
    "                            batch_size=5, verbose=1)\n",
    "\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_params({\n",
    "        \"input_shape\": (1, 5),\n",
    "        \"lstm_units\": 30,\n",
    "        \"dense_layers\": [60, 30, 15],\n",
    "        \"activation\": \"relu\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss_function\": \"mean_squared_error\",\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 5\n",
    "    })\n",
    "\n",
    "    # Registrar la estructura del modelo en MLflow\n",
    "    mlflow.keras.log_model(model_rnn, \"keras_tf_3_21_24\")\n",
    "\n",
    "    # Calcular métricas\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    train_rmse = np.sqrt(train_loss)\n",
    "    test_rmse = np.sqrt(test_loss)\n",
    "\n",
    "    # Registrar métricas de RMSE\n",
    "    mlflow.log_metric(\"train_RMSE\", train_rmse[-1])\n",
    "    mlflow.log_metric(\"test_RMSE\", test_rmse[-1])\n",
    "\n",
    "    # Crear gráfico de pérdida durante el entrenamiento\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.plot(epochs, train_loss, 'bo', label='Pérdida en entrenamiento')\n",
    "    plt.plot(epochs, test_loss, 'r', label='Pérdida en validación')\n",
    "    plt.title('Pérdida durante el entrenamiento y la validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"kerasplot_3_21_24.png\")\n",
    "    mlflow.log_artifact(\"kerasplot_3_21_24.png\")\n",
    "\n",
    "    # Obtener la ruta del modelo registrado\n",
    "    model_path = mlflow.get_artifact_uri(\"keras_tf_3_21_24\")\n",
    "\n",
    "# Registrar el modelo en MLflow\n",
    "model_name = \"MLops_kerastf_3_21_24\"\n",
    "mlflow.register_model(model_path, model_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.model_developer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
