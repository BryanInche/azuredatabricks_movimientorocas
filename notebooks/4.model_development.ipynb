{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ee70a6-a9bd-4886-be5c-f42338b9cef8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/processed/datmarcobre_featureengineer_delta\")\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3142e1-0deb-446f-a302-1ae729e935e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos = df_delta.toPandas()\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ea9309-2b3b-4772-9e17-77be120a2d31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow hyperopt fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063e75e3-48a1-4e46-a7f1-9c361ee98c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "\n",
    "#gf\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.tensorflow\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b39fee1-d9c3-4b3f-9ec7-402b68f3c1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos['numero_pases_carguio'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d5c893-33d6-41d4-b0ae-65ba57e03573",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Supongamos que datos es tu DataFrame y has seleccionado tus características (X) y variable objetivo (y)\n",
    "X = datos[['capacidad_en_volumen_equipo_carguio_m3',\n",
    "'capacidad_en_peso_equipo_carguio',\n",
    "'capacidad_en_peso_equipo_acarreo',\n",
    "'densidad_inicial_poligono_creado_tn/m3',\n",
    "'capacidad_en_volumen_equipo_acarreo_m3']].values\n",
    "\n",
    "y = datos['numero_pases_carguio'].values # Reemplaza 'variable_objetivo' con el nombre de tu variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b517693-84f5-4b77-981b-035f51a5c98d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e5b7a7f-b6b5-4623-afa7-d07c35116c33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 1. Train a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c276c2c-b965-4814-ba1c-f39e88032087",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MLflow Tracking\n",
    "[MLflow tracking](https://www.mlflow.org/docs/latest/tracking.html) allows you to organize your machine learning training code, parameters, and models. \n",
    "\n",
    "You can enable automatic MLflow tracking by using [*autologging*](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4386cf16-81d9-4744-ade2-c6fce039c92f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Establecer semillas en NumPy y TensorFlow\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e08ce29d-a380-4b0f-aaff-1dbfbd6aa478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable MLflow autologging for this notebook\n",
    "#mlflow.autolog()  # Sklearn\n",
    "mlflow.tensorflow.autolog()  # Keras y TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52327a5a-c038-4aa0-b324-57be115e2a37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Modelo 1 - Redes LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3d9399-c2dd-4751-b3f6-b764be691e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias para redes neuronales(LSTM)\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Establecer semillas en NumPy y TensorFlow\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X_array, y_array, test_size=0.2, random_state=42)\n",
    "\n",
    "model_rnn = Sequential()\n",
    "# La forma de entrada para LSTM debe ser (n_timesteps, n_features)\n",
    "model_rnn.add(LSTM(30, activation='relu', input_shape=(1, 5)))  #  (n_samples, n_pasos, n_variables)\n",
    "# Agregar capas Dense según sea necesario\n",
    "model_rnn.add(Dense(60, activation='relu'))\n",
    "model_rnn.add(Dense(30, activation='relu'))\n",
    "model_rnn.add(Dense(15, activation='relu'))\n",
    "# Capa de salida\n",
    "model_rnn.add(Dense(1, activation='linear'))\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compilar el modelo\n",
    "model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# Imprimir un resumen del modelo\n",
    "model_rnn.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model_rnn.fit(X_train_rnn.reshape(-1, 1, 5), y_train_rnn, epochs=1, validation_data=(X_test_rnn.reshape(-1, 1, 5),y_test_rnn), batch_size=5, verbose=1)\n",
    "\n",
    "# VALIDACION 1\n",
    "# Obtener la pérdida en el conjunto de entrenamiento y el conjunto de validación\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Calcular el RMSE en el conjunto de entrenamiento y el conjunto de validación\n",
    "train_rmse = np.sqrt(train_loss)\n",
    "val_rmse = np.sqrt(val_loss)\n",
    "\n",
    "# VALIDACION 2\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred_rnn = model_rnn.predict(X_test_rnn.reshape(-1, 1, 5)) # (n_samples, n_pasos, n_variables)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_rnn = np.round(y_pred_rnn).astype('int64')\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse = mean_squared_error(y_test_rnn, y_pred_rnn)\n",
    "# Calcular la raíz cuadrada del error cuadrático medio\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c282a990-f59d-442d-bc7a-bae8c095f794",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f401675-b7de-4afd-a9f3-30ffb530738b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8764ef5f-a1da-44a1-ad9f-81dd9fd0fd06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlflow.tensorflow.autolog()  # Keras y TensorFlow\n",
    "\n",
    "# Entrenar el modelo\n",
    "with mlflow.start_run(run_name='experimento_mlflow_keras_tf_v3'):  #Iniciliza un nuevo experimento de MlFlow\n",
    "    # Definir y compilar el modelo\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(LSTM(30, activation='relu', input_shape=(1, 5)))\n",
    "    model_rnn.add(Dense(60, activation='relu'))\n",
    "    model_rnn.add(Dense(30, activation='relu'))\n",
    "    model_rnn.add(Dense(15, activation='relu'))\n",
    "    model_rnn.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Entrenar el modelo y guardar el historial del entrenamiento\n",
    "    history = model_rnn.fit(X_train_rnn.reshape(-1, 1, 5), y_train_rnn, epochs=1, validation_data=(X_test_rnn.reshape(-1, 1, 5), y_test_rnn), batch_size=5, verbose=1)\n",
    "\n",
    "    # Save the run information to register the model later\n",
    "    #kerasURI = run.info.artifact_uri\n",
    "\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_params({\n",
    "        \"input_shape\": (1, 5),\n",
    "        \"lstm_units\": 30,\n",
    "        \"dense_layers\": [60, 30, 15],\n",
    "        \"activation\": \"relu\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss_function\": \"mean_squared_error\",\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 5\n",
    "    })\n",
    "\n",
    "    # Registrar la estructura del modelo en MLflow\n",
    "    #mlflow.keras.log_model(model_rnn, \"keras_tf_v1\")\n",
    "\n",
    "    # VALIDACION \n",
    "    # Obtener la pérdida en el conjunto de entrenamiento y el conjunto de validación\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "\n",
    "    # Calcular el RMSE en el conjunto de entrenamiento y el conjunto de validación\n",
    "    train_rmse = np.sqrt(train_loss)\n",
    "    test_rmse = np.sqrt(test_loss)\n",
    "\n",
    "    # Registro de métricas de RMSE\n",
    "    mlflow.log_metric(\"train_RMSE\", train_rmse[-1])  # Se registra el último valor de RMSE del conjunto de entrenamiento\n",
    "    mlflow.log_metric(\"test_RMSE\", test_rmse[-1])      # Se registra el último valor de RMSE del conjunto de validación\n",
    "    \n",
    "    # Crear gráfico de pérdida durante el entrenamiento\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.plot(epochs, train_loss, 'bo', label='Pérdida en entrenamiento')\n",
    "    plt.plot(epochs, test_loss, 'r', label='Pérdida en validación')\n",
    "    plt.title('Pérdida durante el entrenamiento y la validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"kerasplotv3.png\")\n",
    "    mlflow.log_artifact(\"kerasplotv3.png\")\n",
    "\n",
    "    # Registrar el modelo en MLflow y obtener el model_path\n",
    "    mlflow.keras.log_model(model_rnn, \"keras_tf_v3\")\n",
    "    \n",
    "    # Obtener el model_path del modelo registrado\n",
    "    model_path = mlflow.get_artifact_uri(\"keras_tf_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f71112-0887-4995-8f0d-c0d4630afdfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f51adca-18d1-47bf-aefb-822358ab3326",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar el modelo en MLflow utilizando el model_path obtenido\n",
    "\n",
    "# dbfs:/databricks/mlflow-tracking/8323b80e926241ee8763b7dc43085602/186379e93c684b8dbe8e65cb53cf653a/artifacts/model\n",
    "# o esto \n",
    "# dbfs:/databricks/mlflow-tracking/8323b80e926241ee8763b7dc43085602/186379e93c684b8dbe8e65cb53cf653a/artifacts/keras_tf_v3  son iguales\n",
    "model_name = \"MLops_kerastf_v3\"\n",
    "mlflow.register_model(model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ce886b-1c30-46b8-b132-11dc59e7f55c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Crear gráfico de pérdida durante el entrenamiento\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label='Pérdida en entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'r', label='Pérdida en validación')\n",
    "plt.title('Pérdida durante el entrenamiento y la validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3382b8d0-4ee8-47d8-8b6c-3ed5fd7a3ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Modelo 2 : Redes Tradicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9853087a-3f70-496b-8680-adb720259e22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias para redes neuronales(LSTM)\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 3. Construccion del modelo de RED NEURONAL\n",
    "# Aseguramos que los resultados sean \"reproducibles\" en cada ejecucion de tensorflow(pesos iniciales aleatorios)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Arquitectura 2:\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Dense(15, activation='relu', input_shape=[5]))\n",
    "model_rnn.add(Dense(7, activation='relu'))\n",
    "model_rnn.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# Imprimir un resumen del modelo\n",
    "model_rnn.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "# Red LSTM\n",
    "#history = model_rnn.fit(X_train_rnn.reshape(-1, 1, 5), y_train_rnn, epochs=5, validation_data=(X_test_rnn.reshape(-1, 1, 5),y_test_rnn), batch_size=5, verbose=1)\n",
    "# Red Tradicional\n",
    "history = model_rnn.fit(X_train_rnn, y_train_rnn, epochs=4, validation_data=(X_test_rnn,y_test_rnn), batch_size=5, verbose=1)\n",
    "\n",
    "# 4. Validacion del Modelo con la metrica de RSME\n",
    "# VALIDACION 1 del Modelo\n",
    "# Obtener la pérdida en el conjunto de entrenamiento y el conjunto de validación\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Calcular el RMSE en el conjunto de entrenamiento y el conjunto de validación\n",
    "train_rmse = np.sqrt(train_loss)\n",
    "val_rmse = np.sqrt(val_loss)\n",
    "\n",
    "# VALIDACION 2 del Modelo\n",
    "# Hacer predicciones en el conjunto de prueba Red LSTM\n",
    "#y_pred_rnn = model_rnn.predict(X_test_rnn.reshape(-1, 1, 5)) # (n_samples, n_pasos, n_variables)\n",
    "# Hacer predicciones en el conjunto de prueba Red Tradicional\n",
    "y_pred_rnn = model_rnn.predict(X_test_rnn) # (n_samples, n_pasos, n_variables)\n",
    "\n",
    "# Redondear los valores de y_pred al entero más cercano\n",
    "y_pred_rnn = np.round(y_pred_rnn).astype('int64')\n",
    "\n",
    "# Calcular el error cuadrático medio en el conjunto de prueba\n",
    "mse = mean_squared_error(y_test_rnn, y_pred_rnn)\n",
    "# Calcular la raíz cuadrada del error cuadrático medio\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Raíz del Error Cuadrático Medio en el conjunto de prueba: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d303d06d-4296-4cbf-9b94-c563c6d2b313",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Crear gráfico de pérdida durante el entrenamiento\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label='Pérdida en entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'r', label='Pérdida en validación')\n",
    "plt.title('Pérdida durante el entrenamiento y la validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532ddb61-3f2c-4052-87e3-24ef6df8efb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_pred_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b274f1da-e5a1-4bc8-90ce-f342a61bfd64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar un modelo en MLflow\n",
    "model_path = \"dbfs:/databricks/mlflow-tracking/8323b80e926241ee8763b7dc43085602/3fdcef4bd83943ec9dc2188bc13045f4/artifacts/model\"\n",
    "model_name = \"MLops_kerastf_v1\"\n",
    "mlflow.register_model(model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af3a01f-943e-4409-a614-592e3ecbd897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ver el directorio local de databrinks mas no el local de .os\n",
    "dbutils.fs.ls(\"dbfs:/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df2356b-8582-4906-b953-ea2bfe0cb6ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Guardar Modelo en Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621d2c95-ecd2-4658-9469-93f93af1af77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Guardar en el directorio LOCAL (.OS)\n",
    "model_rnn.save('/nuevomodelo/modelrnn_nuevo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "349ed379-807a-458e-b837-b4a138358ce4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Verificar que se ha guardado en el directorio LOCAL (.OS)\n",
    "import os\n",
    "os.listdir(\"/modelornn_version1_d4m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe3bae1-24bd-403d-814c-b33f777e6516",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Opcion 1: Cargar el Modelo ML directamente del Directorio (.OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2765c5c1-8d8e-4046-94d0-e98a024dd08b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo desde el contenedor de almacenamiento Azure\n",
    "loaded_model_nuevo = tf.keras.models.load_model(\"/nuevomodelo/modelrnn_nuevo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2638681c-a02a-426e-94a6-2f5f05ed0417",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Opcion  2: Copiar el Modelo ML desde el Directorio LOCAL(.OS)  HASTA  Directorio de DBFS de Azure DataBrinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6020d49-aa50-46a7-8d59-618603e2d66b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#copie los datos desde LOCAL(.OS) a DBFS como dbfs (DIRECTORIO DE DATABRINKS)\n",
    "dbutils.fs.cp(\"file:/nuevomodelo/modelrnn_nuevo\", \"dbfs:/nuevomodelo/modelrnn_nuevo\", recurse=True) # True: si copias una carpeta, False: si #solo es archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d7cef4-3479-4c9d-a1d8-674dfb543d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Verificar que se ha migrado desde Directorio LOCAL(.OS)  Hacia Directorio DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec38fcb3-0103-4181-b59e-303202ad9ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/nuevomodelo/modelrnn_nuevo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb1bdbeb-327b-41b5-b064-35f2e75a04a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Copiar el modelo desde el directorio DBFS al contenedor de almacenamiento Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7d3187-f0c7-498a-88a7-5960e258d7d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copiar el modelo desde el directorio DBFS al contenedor de almacenamiento Azure Storage\n",
    "dbutils.fs.cp(\"/nuevomodelo/modelrnn_nuevo\", \"/mnt/datalakemlopsd4m/presentation/modelrnn_nuevo\", recurse=True) #True:Copycarpeta,#False:Copyarchivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68462a76-164b-4caf-ae33-62dd7be082c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Establecer la URI de seguimiento de MLflow\n",
    "# mlflow.set_tracking_uri(\"databricks\")  # Usa \"databricks\" para Databricks\n",
    "\n",
    "# # Crear un nuevo experimento si no existe\n",
    "# experiment_name = \"nombre_del_experimento\"\n",
    "# mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Cargar los datos\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/presentation/datmarcobre_fengineer_tablacaract_delta\")\n",
    "datos = df_delta.toPandas()\n",
    "\n",
    "# Separar las variables independientes y dependiente\n",
    "X = datos[['capacidad_en_volumen_equipo_carguio_m3',\n",
    "           'capacidad_en_peso_equipo_carguio',\n",
    "           'capacidad_en_peso_equipo_acarreo',\n",
    "           'densidad_inicial_poligono_creado_tn/m3',\n",
    "           'capacidad_en_volumen_equipo_acarreo_m3']].values\n",
    "y = datos['numero_pases_carguio'].values\n",
    "\n",
    "# Construir el modelo de red neuronal\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mlflow.tensorflow.autolog()  # Keras y TensorFlow\n",
    "\n",
    "# Entrenar el modelo\n",
    "with mlflow.start_run(run_name='experimento_mlflow_keras_tf_3_21_24'):  #Iniciliza un nuevo experimento de MlFlow\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(LSTM(30, activation='relu', input_shape=(1, 5)))\n",
    "    model_rnn.add(Dense(60, activation='relu'))\n",
    "    model_rnn.add(Dense(30, activation='relu'))\n",
    "    model_rnn.add(Dense(15, activation='relu'))\n",
    "    model_rnn.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    history = model_rnn.fit(X_train_rnn.reshape(-1, 1, 5), y_train_rnn, epochs=1, \n",
    "                            validation_data=(X_test_rnn.reshape(-1, 1, 5), y_test_rnn), \n",
    "                            batch_size=5, verbose=1)\n",
    "\n",
    "    # Registrar los parámetros del modelo\n",
    "    mlflow.log_params({\n",
    "        \"input_shape\": (1, 5),\n",
    "        \"lstm_units\": 30,\n",
    "        \"dense_layers\": [60, 30, 15],\n",
    "        \"activation\": \"relu\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss_function\": \"mean_squared_error\",\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 5\n",
    "    })\n",
    "\n",
    "    # Registrar la estructura del modelo en MLflow\n",
    "    mlflow.keras.log_model(model_rnn, \"keras_tf_3_21_24\")\n",
    "\n",
    "    # Calcular métricas\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    train_rmse = np.sqrt(train_loss)\n",
    "    test_rmse = np.sqrt(test_loss)\n",
    "\n",
    "    # Registrar métricas de RMSE\n",
    "    mlflow.log_metric(\"train_RMSE\", train_rmse[-1])\n",
    "    mlflow.log_metric(\"test_RMSE\", test_rmse[-1])\n",
    "\n",
    "    # Crear gráfico de pérdida durante el entrenamiento\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.plot(epochs, train_loss, 'bo', label='Pérdida en entrenamiento')\n",
    "    plt.plot(epochs, test_loss, 'r', label='Pérdida en validación')\n",
    "    plt.title('Pérdida durante el entrenamiento y la validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"kerasplot_3_21_24.png\")\n",
    "    mlflow.log_artifact(\"kerasplot_3_21_24.png\")\n",
    "\n",
    "    # Obtener la ruta del modelo registrado\n",
    "    model_path = mlflow.get_artifact_uri(\"keras_tf_3_21_24\")\n",
    "\n",
    "# Registrar el modelo en MLflow\n",
    "model_name = \"MLops_kerastf_3_21_24\"\n",
    "mlflow.register_model(model_path, model_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.model_development",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
